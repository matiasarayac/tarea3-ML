{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3 - Pregunta 4 - Reconocimiento de Imágenes Sign Gestures\n",
    "---\n",
    "# Matías Araya - 201173082-8\n",
    "# Claudia Hazard - 201404523-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "Se cargan los datos de entrenamiento y prueba del problema generando matrices de prueba con 27455 imagenes, entrenamiento con  7172 y validación con 6864 imagenes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27455\n",
      "7172\n",
      "6864\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_data():\n",
    "    train = pd.read_csv('sign_mnist_train.csv')\n",
    "    test = pd.read_csv('sign_mnist_test.csv')\n",
    "    y_tr = train['label']\n",
    "    x_tr = train.iloc[:,1:]\n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:,1:]\n",
    "    _,x_v,_,y_v = train_test_split(x_tr, y_tr, test_size=0.25, random_state=42)\n",
    "    return(x_tr,x_v,x_t,y_tr,y_v,y_t)\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t= load_data()\n",
    "print(len(x_tr))\n",
    "print(len(x_t))\n",
    "print(len(x_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)\n",
    " Se escala las imagenes para poder trabajar con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.928951</td>\n",
       "      <td>-0.763625</td>\n",
       "      <td>-0.620852</td>\n",
       "      <td>-0.506458</td>\n",
       "      <td>-0.463774</td>\n",
       "      <td>-0.426610</td>\n",
       "      <td>-0.413304</td>\n",
       "      <td>-0.366583</td>\n",
       "      <td>-0.335512</td>\n",
       "      <td>-0.304798</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033649</td>\n",
       "      <td>0.908302</td>\n",
       "      <td>0.833111</td>\n",
       "      <td>0.751478</td>\n",
       "      <td>0.690817</td>\n",
       "      <td>0.681926</td>\n",
       "      <td>0.678557</td>\n",
       "      <td>0.664063</td>\n",
       "      <td>0.658113</td>\n",
       "      <td>0.654939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.231652</td>\n",
       "      <td>0.212805</td>\n",
       "      <td>0.121680</td>\n",
       "      <td>0.063570</td>\n",
       "      <td>-0.005683</td>\n",
       "      <td>-0.039066</td>\n",
       "      <td>-0.127718</td>\n",
       "      <td>-0.128922</td>\n",
       "      <td>-0.182377</td>\n",
       "      <td>-0.272827</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.131056</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>-0.393098</td>\n",
       "      <td>-1.132135</td>\n",
       "      <td>-1.066399</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>-0.931575</td>\n",
       "      <td>-0.410913</td>\n",
       "      <td>-0.168097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.005388</td>\n",
       "      <td>0.988941</td>\n",
       "      <td>0.941025</td>\n",
       "      <td>0.866793</td>\n",
       "      <td>0.829661</td>\n",
       "      <td>0.763704</td>\n",
       "      <td>0.757597</td>\n",
       "      <td>0.762307</td>\n",
       "      <td>0.705804</td>\n",
       "      <td>0.654322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955217</td>\n",
       "      <td>0.816715</td>\n",
       "      <td>0.724459</td>\n",
       "      <td>0.625904</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>0.571590</td>\n",
       "      <td>0.552589</td>\n",
       "      <td>0.521877</td>\n",
       "      <td>0.516625</td>\n",
       "      <td>0.546237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.585689</td>\n",
       "      <td>1.564785</td>\n",
       "      <td>1.555534</td>\n",
       "      <td>1.514553</td>\n",
       "      <td>1.476378</td>\n",
       "      <td>1.428065</td>\n",
       "      <td>1.443003</td>\n",
       "      <td>1.415875</td>\n",
       "      <td>1.410223</td>\n",
       "      <td>1.453589</td>\n",
       "      <td>...</td>\n",
       "      <td>1.472864</td>\n",
       "      <td>1.320442</td>\n",
       "      <td>1.236673</td>\n",
       "      <td>1.128201</td>\n",
       "      <td>1.067364</td>\n",
       "      <td>0.997170</td>\n",
       "      <td>0.977732</td>\n",
       "      <td>0.948434</td>\n",
       "      <td>1.066859</td>\n",
       "      <td>0.049309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449265</td>\n",
       "      <td>0.463171</td>\n",
       "      <td>0.480144</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>0.533248</td>\n",
       "      <td>0.569932</td>\n",
       "      <td>0.557687</td>\n",
       "      <td>0.643477</td>\n",
       "      <td>0.644550</td>\n",
       "      <td>0.654322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.770272</td>\n",
       "      <td>-0.648672</td>\n",
       "      <td>-0.750095</td>\n",
       "      <td>-0.802503</td>\n",
       "      <td>-0.454511</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>-0.092998</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.044995</td>\n",
       "      <td>0.297773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.376728</td>\n",
       "      <td>0.488208</td>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.504047</td>\n",
       "      <td>0.587142</td>\n",
       "      <td>0.708341</td>\n",
       "      <td>0.814715</td>\n",
       "      <td>0.910845</td>\n",
       "      <td>0.981446</td>\n",
       "      <td>1.165853</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.021252</td>\n",
       "      <td>-1.121870</td>\n",
       "      <td>-1.324395</td>\n",
       "      <td>-1.524555</td>\n",
       "      <td>-1.709666</td>\n",
       "      <td>-1.698169</td>\n",
       "      <td>-1.809316</td>\n",
       "      <td>1.201208</td>\n",
       "      <td>1.475604</td>\n",
       "      <td>1.477976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.276112</td>\n",
       "      <td>-0.363038</td>\n",
       "      <td>-0.416015</td>\n",
       "      <td>-0.480548</td>\n",
       "      <td>-0.544614</td>\n",
       "      <td>-0.592700</td>\n",
       "      <td>-0.670331</td>\n",
       "      <td>-0.723074</td>\n",
       "      <td>-0.794916</td>\n",
       "      <td>-0.880270</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503605</td>\n",
       "      <td>-0.694465</td>\n",
       "      <td>-0.967398</td>\n",
       "      <td>-1.477464</td>\n",
       "      <td>-0.376064</td>\n",
       "      <td>0.413968</td>\n",
       "      <td>0.253415</td>\n",
       "      <td>0.300699</td>\n",
       "      <td>0.312252</td>\n",
       "      <td>0.297773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.759696</td>\n",
       "      <td>-2.666411</td>\n",
       "      <td>-1.977892</td>\n",
       "      <td>-1.413322</td>\n",
       "      <td>-1.406904</td>\n",
       "      <td>-1.367789</td>\n",
       "      <td>-1.241503</td>\n",
       "      <td>-1.049858</td>\n",
       "      <td>-0.672408</td>\n",
       "      <td>-0.432681</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143453</td>\n",
       "      <td>1.076211</td>\n",
       "      <td>1.034892</td>\n",
       "      <td>1.002626</td>\n",
       "      <td>0.957538</td>\n",
       "      <td>0.981408</td>\n",
       "      <td>1.009224</td>\n",
       "      <td>1.027426</td>\n",
       "      <td>1.051138</td>\n",
       "      <td>1.058693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.570161</td>\n",
       "      <td>0.638428</td>\n",
       "      <td>0.633771</td>\n",
       "      <td>0.685420</td>\n",
       "      <td>0.721874</td>\n",
       "      <td>0.736022</td>\n",
       "      <td>0.757597</td>\n",
       "      <td>0.762307</td>\n",
       "      <td>0.797684</td>\n",
       "      <td>0.814175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346743</td>\n",
       "      <td>-0.450234</td>\n",
       "      <td>-0.470706</td>\n",
       "      <td>-0.614142</td>\n",
       "      <td>-0.689853</td>\n",
       "      <td>-0.768198</td>\n",
       "      <td>-1.084998</td>\n",
       "      <td>-1.389729</td>\n",
       "      <td>-1.574266</td>\n",
       "      <td>-1.596765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.053746</td>\n",
       "      <td>1.013978</td>\n",
       "      <td>0.966630</td>\n",
       "      <td>0.944524</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>0.902113</td>\n",
       "      <td>0.843273</td>\n",
       "      <td>0.821722</td>\n",
       "      <td>0.797684</td>\n",
       "      <td>0.750234</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.009487</td>\n",
       "      <td>-1.442423</td>\n",
       "      <td>0.724459</td>\n",
       "      <td>0.704388</td>\n",
       "      <td>0.612370</td>\n",
       "      <td>0.603114</td>\n",
       "      <td>0.473859</td>\n",
       "      <td>0.205909</td>\n",
       "      <td>0.265089</td>\n",
       "      <td>-0.059394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.300291</td>\n",
       "      <td>-0.338002</td>\n",
       "      <td>-0.262388</td>\n",
       "      <td>-0.195534</td>\n",
       "      <td>-0.167362</td>\n",
       "      <td>-0.094429</td>\n",
       "      <td>-0.070601</td>\n",
       "      <td>-0.099214</td>\n",
       "      <td>-0.029243</td>\n",
       "      <td>-0.017062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660468</td>\n",
       "      <td>-0.786052</td>\n",
       "      <td>-0.889790</td>\n",
       "      <td>-0.975167</td>\n",
       "      <td>-1.035020</td>\n",
       "      <td>-1.067680</td>\n",
       "      <td>-1.084998</td>\n",
       "      <td>-1.073761</td>\n",
       "      <td>-1.071195</td>\n",
       "      <td>-1.053250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.516129</td>\n",
       "      <td>-3.092035</td>\n",
       "      <td>-2.899656</td>\n",
       "      <td>-2.942036</td>\n",
       "      <td>-3.104537</td>\n",
       "      <td>-3.111737</td>\n",
       "      <td>-3.154927</td>\n",
       "      <td>-3.159100</td>\n",
       "      <td>-2.908173</td>\n",
       "      <td>-2.702598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189880</td>\n",
       "      <td>-0.953961</td>\n",
       "      <td>-1.448568</td>\n",
       "      <td>-1.493161</td>\n",
       "      <td>-1.411567</td>\n",
       "      <td>-1.461736</td>\n",
       "      <td>-1.510141</td>\n",
       "      <td>-1.531914</td>\n",
       "      <td>-1.558545</td>\n",
       "      <td>-1.488062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.412536</td>\n",
       "      <td>-1.439615</td>\n",
       "      <td>-1.337779</td>\n",
       "      <td>-0.972846</td>\n",
       "      <td>-0.652400</td>\n",
       "      <td>-0.454292</td>\n",
       "      <td>-0.384746</td>\n",
       "      <td>-0.277460</td>\n",
       "      <td>-0.121123</td>\n",
       "      <td>-0.049033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547374</td>\n",
       "      <td>0.679335</td>\n",
       "      <td>-0.393098</td>\n",
       "      <td>-2.152426</td>\n",
       "      <td>-2.494138</td>\n",
       "      <td>-2.502042</td>\n",
       "      <td>-2.502142</td>\n",
       "      <td>-2.543012</td>\n",
       "      <td>-2.501804</td>\n",
       "      <td>-2.435330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.581790</td>\n",
       "      <td>-1.264358</td>\n",
       "      <td>-0.774479</td>\n",
       "      <td>-2.967946</td>\n",
       "      <td>-2.781178</td>\n",
       "      <td>-1.783014</td>\n",
       "      <td>-1.726998</td>\n",
       "      <td>-1.644011</td>\n",
       "      <td>-1.652469</td>\n",
       "      <td>-1.199976</td>\n",
       "      <td>...</td>\n",
       "      <td>1.457178</td>\n",
       "      <td>1.244120</td>\n",
       "      <td>1.252195</td>\n",
       "      <td>1.190988</td>\n",
       "      <td>1.192879</td>\n",
       "      <td>1.233603</td>\n",
       "      <td>1.245415</td>\n",
       "      <td>1.295998</td>\n",
       "      <td>1.334116</td>\n",
       "      <td>1.338215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.445366</td>\n",
       "      <td>-0.538295</td>\n",
       "      <td>-0.595247</td>\n",
       "      <td>-0.610100</td>\n",
       "      <td>-0.652400</td>\n",
       "      <td>-0.703427</td>\n",
       "      <td>-0.784566</td>\n",
       "      <td>-0.871612</td>\n",
       "      <td>-0.886796</td>\n",
       "      <td>-0.944211</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.444781</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>-0.127542</td>\n",
       "      <td>-0.187791</td>\n",
       "      <td>-0.216521</td>\n",
       "      <td>-0.250459</td>\n",
       "      <td>-0.268043</td>\n",
       "      <td>-0.269425</td>\n",
       "      <td>-0.261271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.436715</td>\n",
       "      <td>-1.539762</td>\n",
       "      <td>-1.593824</td>\n",
       "      <td>-1.568785</td>\n",
       "      <td>-1.406904</td>\n",
       "      <td>-1.229380</td>\n",
       "      <td>-1.098710</td>\n",
       "      <td>-0.931028</td>\n",
       "      <td>-0.825543</td>\n",
       "      <td>-0.816329</td>\n",
       "      <td>...</td>\n",
       "      <td>1.739531</td>\n",
       "      <td>1.473086</td>\n",
       "      <td>1.314281</td>\n",
       "      <td>1.332259</td>\n",
       "      <td>1.051675</td>\n",
       "      <td>-0.090423</td>\n",
       "      <td>-1.226713</td>\n",
       "      <td>-0.631406</td>\n",
       "      <td>-0.316588</td>\n",
       "      <td>-0.494206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.662980</td>\n",
       "      <td>-0.713552</td>\n",
       "      <td>-0.595247</td>\n",
       "      <td>-0.480548</td>\n",
       "      <td>-0.463774</td>\n",
       "      <td>-0.371247</td>\n",
       "      <td>-0.327628</td>\n",
       "      <td>-0.366583</td>\n",
       "      <td>-0.335512</td>\n",
       "      <td>-0.304798</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331056</td>\n",
       "      <td>-1.045548</td>\n",
       "      <td>-0.827703</td>\n",
       "      <td>-0.174632</td>\n",
       "      <td>-0.187791</td>\n",
       "      <td>-0.137710</td>\n",
       "      <td>-0.045760</td>\n",
       "      <td>0.063724</td>\n",
       "      <td>0.139321</td>\n",
       "      <td>0.313302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.875840</td>\n",
       "      <td>1.915298</td>\n",
       "      <td>1.913998</td>\n",
       "      <td>1.903209</td>\n",
       "      <td>1.934470</td>\n",
       "      <td>1.954018</td>\n",
       "      <td>1.985616</td>\n",
       "      <td>2.010028</td>\n",
       "      <td>2.022761</td>\n",
       "      <td>2.093002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578747</td>\n",
       "      <td>-0.267061</td>\n",
       "      <td>-1.060528</td>\n",
       "      <td>-0.975167</td>\n",
       "      <td>1.318395</td>\n",
       "      <td>1.343939</td>\n",
       "      <td>1.213923</td>\n",
       "      <td>0.995829</td>\n",
       "      <td>1.349837</td>\n",
       "      <td>1.477976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.053746</td>\n",
       "      <td>1.114125</td>\n",
       "      <td>1.120257</td>\n",
       "      <td>1.125897</td>\n",
       "      <td>1.206912</td>\n",
       "      <td>1.317338</td>\n",
       "      <td>1.357327</td>\n",
       "      <td>1.504998</td>\n",
       "      <td>1.563357</td>\n",
       "      <td>1.677383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657178</td>\n",
       "      <td>0.877773</td>\n",
       "      <td>0.134638</td>\n",
       "      <td>-0.457174</td>\n",
       "      <td>0.471165</td>\n",
       "      <td>1.107505</td>\n",
       "      <td>-0.754332</td>\n",
       "      <td>-0.789390</td>\n",
       "      <td>0.610950</td>\n",
       "      <td>0.530708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.666878</td>\n",
       "      <td>0.638428</td>\n",
       "      <td>0.633771</td>\n",
       "      <td>0.607689</td>\n",
       "      <td>0.533248</td>\n",
       "      <td>0.514568</td>\n",
       "      <td>0.472012</td>\n",
       "      <td>0.435523</td>\n",
       "      <td>0.399534</td>\n",
       "      <td>0.366586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955217</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>0.724459</td>\n",
       "      <td>0.641601</td>\n",
       "      <td>0.580991</td>\n",
       "      <td>0.540066</td>\n",
       "      <td>0.505351</td>\n",
       "      <td>0.521877</td>\n",
       "      <td>0.500904</td>\n",
       "      <td>0.468592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.086577</td>\n",
       "      <td>0.037548</td>\n",
       "      <td>-0.031947</td>\n",
       "      <td>-0.091892</td>\n",
       "      <td>-0.167362</td>\n",
       "      <td>-0.205156</td>\n",
       "      <td>-0.270511</td>\n",
       "      <td>-0.336875</td>\n",
       "      <td>-0.396766</td>\n",
       "      <td>-0.464651</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.491840</td>\n",
       "      <td>-2.190381</td>\n",
       "      <td>-1.308874</td>\n",
       "      <td>0.374755</td>\n",
       "      <td>0.078929</td>\n",
       "      <td>0.067199</td>\n",
       "      <td>0.064462</td>\n",
       "      <td>0.032127</td>\n",
       "      <td>0.013553</td>\n",
       "      <td>0.018251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.348650</td>\n",
       "      <td>-0.338002</td>\n",
       "      <td>-0.313597</td>\n",
       "      <td>-0.273265</td>\n",
       "      <td>-0.302095</td>\n",
       "      <td>-0.343565</td>\n",
       "      <td>-0.327628</td>\n",
       "      <td>-0.307167</td>\n",
       "      <td>-0.335512</td>\n",
       "      <td>-0.304798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516002</td>\n",
       "      <td>0.679335</td>\n",
       "      <td>0.584765</td>\n",
       "      <td>0.500330</td>\n",
       "      <td>0.455476</td>\n",
       "      <td>0.445492</td>\n",
       "      <td>0.473859</td>\n",
       "      <td>0.458683</td>\n",
       "      <td>0.453741</td>\n",
       "      <td>0.484121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.339998</td>\n",
       "      <td>-1.264358</td>\n",
       "      <td>-1.184151</td>\n",
       "      <td>-1.128308</td>\n",
       "      <td>-1.029652</td>\n",
       "      <td>-0.952563</td>\n",
       "      <td>-0.984476</td>\n",
       "      <td>-1.049858</td>\n",
       "      <td>-1.009304</td>\n",
       "      <td>-0.944211</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.213408</td>\n",
       "      <td>-0.328119</td>\n",
       "      <td>1.205630</td>\n",
       "      <td>0.767175</td>\n",
       "      <td>0.800644</td>\n",
       "      <td>0.808023</td>\n",
       "      <td>0.820272</td>\n",
       "      <td>0.822047</td>\n",
       "      <td>0.831044</td>\n",
       "      <td>0.856816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.460894</td>\n",
       "      <td>-1.464652</td>\n",
       "      <td>-1.388988</td>\n",
       "      <td>-1.309681</td>\n",
       "      <td>-1.002705</td>\n",
       "      <td>-0.786472</td>\n",
       "      <td>-0.641773</td>\n",
       "      <td>-0.455706</td>\n",
       "      <td>-0.366139</td>\n",
       "      <td>-0.272827</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206198</td>\n",
       "      <td>1.106740</td>\n",
       "      <td>1.050414</td>\n",
       "      <td>1.002626</td>\n",
       "      <td>1.004606</td>\n",
       "      <td>1.028694</td>\n",
       "      <td>1.040716</td>\n",
       "      <td>1.074821</td>\n",
       "      <td>1.082580</td>\n",
       "      <td>1.089751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.348650</td>\n",
       "      <td>-0.262892</td>\n",
       "      <td>-0.185574</td>\n",
       "      <td>-0.195534</td>\n",
       "      <td>-0.221255</td>\n",
       "      <td>-0.232838</td>\n",
       "      <td>-0.241953</td>\n",
       "      <td>-0.277460</td>\n",
       "      <td>-0.304885</td>\n",
       "      <td>-0.304798</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566350</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.320897</td>\n",
       "      <td>0.249181</td>\n",
       "      <td>0.173066</td>\n",
       "      <td>0.130248</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.079522</td>\n",
       "      <td>0.060716</td>\n",
       "      <td>0.049309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.183294</td>\n",
       "      <td>0.212805</td>\n",
       "      <td>0.300912</td>\n",
       "      <td>0.322674</td>\n",
       "      <td>0.371569</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>0.414895</td>\n",
       "      <td>0.435523</td>\n",
       "      <td>0.430161</td>\n",
       "      <td>0.430527</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049335</td>\n",
       "      <td>0.908302</td>\n",
       "      <td>0.817589</td>\n",
       "      <td>0.735781</td>\n",
       "      <td>0.675128</td>\n",
       "      <td>0.650401</td>\n",
       "      <td>0.647065</td>\n",
       "      <td>0.648264</td>\n",
       "      <td>0.642392</td>\n",
       "      <td>0.623881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.062397</td>\n",
       "      <td>0.112658</td>\n",
       "      <td>0.121680</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>0.048210</td>\n",
       "      <td>-0.011384</td>\n",
       "      <td>-0.070601</td>\n",
       "      <td>-0.128922</td>\n",
       "      <td>-0.121123</td>\n",
       "      <td>-0.176915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672865</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.476113</td>\n",
       "      <td>0.343362</td>\n",
       "      <td>0.298582</td>\n",
       "      <td>0.256346</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.155042</td>\n",
       "      <td>0.111425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.179395</td>\n",
       "      <td>-0.187782</td>\n",
       "      <td>-0.211179</td>\n",
       "      <td>-0.169623</td>\n",
       "      <td>-0.194309</td>\n",
       "      <td>-0.232838</td>\n",
       "      <td>-0.299070</td>\n",
       "      <td>-0.336875</td>\n",
       "      <td>-0.304885</td>\n",
       "      <td>-0.368739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531688</td>\n",
       "      <td>0.419839</td>\n",
       "      <td>0.351940</td>\n",
       "      <td>0.280575</td>\n",
       "      <td>0.251513</td>\n",
       "      <td>0.240583</td>\n",
       "      <td>0.221922</td>\n",
       "      <td>0.237506</td>\n",
       "      <td>0.265089</td>\n",
       "      <td>0.282244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.775224</td>\n",
       "      <td>-1.715018</td>\n",
       "      <td>-1.645033</td>\n",
       "      <td>-1.594695</td>\n",
       "      <td>-1.622476</td>\n",
       "      <td>-1.589242</td>\n",
       "      <td>-1.584206</td>\n",
       "      <td>-1.584596</td>\n",
       "      <td>-1.591216</td>\n",
       "      <td>-1.583624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.738899</td>\n",
       "      <td>-1.198192</td>\n",
       "      <td>-0.967398</td>\n",
       "      <td>0.107910</td>\n",
       "      <td>-0.046586</td>\n",
       "      <td>-0.058899</td>\n",
       "      <td>-0.077252</td>\n",
       "      <td>-0.078462</td>\n",
       "      <td>-0.080773</td>\n",
       "      <td>-0.090452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.014039</td>\n",
       "      <td>0.012511</td>\n",
       "      <td>-0.031947</td>\n",
       "      <td>-0.091892</td>\n",
       "      <td>-0.140416</td>\n",
       "      <td>-0.177475</td>\n",
       "      <td>-0.241953</td>\n",
       "      <td>-0.277460</td>\n",
       "      <td>-0.335512</td>\n",
       "      <td>-0.368739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>0.755657</td>\n",
       "      <td>0.677895</td>\n",
       "      <td>0.578813</td>\n",
       "      <td>0.533923</td>\n",
       "      <td>0.508541</td>\n",
       "      <td>0.489605</td>\n",
       "      <td>0.506079</td>\n",
       "      <td>0.453741</td>\n",
       "      <td>0.422005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27425</th>\n",
       "      <td>-0.735517</td>\n",
       "      <td>-0.663478</td>\n",
       "      <td>-0.646456</td>\n",
       "      <td>-0.610100</td>\n",
       "      <td>-0.571561</td>\n",
       "      <td>-0.565019</td>\n",
       "      <td>-0.584656</td>\n",
       "      <td>-0.574536</td>\n",
       "      <td>-0.611154</td>\n",
       "      <td>-0.624505</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.868311</td>\n",
       "      <td>0.480897</td>\n",
       "      <td>0.460592</td>\n",
       "      <td>0.311968</td>\n",
       "      <td>0.267203</td>\n",
       "      <td>0.224821</td>\n",
       "      <td>0.206176</td>\n",
       "      <td>0.205909</td>\n",
       "      <td>0.170763</td>\n",
       "      <td>0.142483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27426</th>\n",
       "      <td>0.473444</td>\n",
       "      <td>0.513245</td>\n",
       "      <td>0.505748</td>\n",
       "      <td>0.529958</td>\n",
       "      <td>0.506302</td>\n",
       "      <td>0.514568</td>\n",
       "      <td>0.529129</td>\n",
       "      <td>0.524646</td>\n",
       "      <td>0.583296</td>\n",
       "      <td>0.558410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092473</td>\n",
       "      <td>1.152533</td>\n",
       "      <td>0.926241</td>\n",
       "      <td>0.845659</td>\n",
       "      <td>0.800644</td>\n",
       "      <td>0.792261</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.758853</td>\n",
       "      <td>0.768160</td>\n",
       "      <td>0.748113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27427</th>\n",
       "      <td>0.787775</td>\n",
       "      <td>0.738575</td>\n",
       "      <td>0.710584</td>\n",
       "      <td>0.659510</td>\n",
       "      <td>0.587142</td>\n",
       "      <td>0.542250</td>\n",
       "      <td>0.472012</td>\n",
       "      <td>0.405816</td>\n",
       "      <td>0.399534</td>\n",
       "      <td>0.334615</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.648703</td>\n",
       "      <td>-1.686655</td>\n",
       "      <td>-1.914217</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.518234</td>\n",
       "      <td>0.398206</td>\n",
       "      <td>0.379383</td>\n",
       "      <td>0.395490</td>\n",
       "      <td>0.390857</td>\n",
       "      <td>0.390947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27428</th>\n",
       "      <td>-1.146564</td>\n",
       "      <td>-1.139175</td>\n",
       "      <td>-1.056129</td>\n",
       "      <td>-0.998756</td>\n",
       "      <td>-1.002705</td>\n",
       "      <td>-0.952563</td>\n",
       "      <td>-0.984476</td>\n",
       "      <td>-0.960735</td>\n",
       "      <td>-0.978677</td>\n",
       "      <td>-1.008153</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.287919</td>\n",
       "      <td>-1.198192</td>\n",
       "      <td>-1.122614</td>\n",
       "      <td>-1.242013</td>\n",
       "      <td>-1.301741</td>\n",
       "      <td>-1.225303</td>\n",
       "      <td>-0.911792</td>\n",
       "      <td>-0.726196</td>\n",
       "      <td>-0.410913</td>\n",
       "      <td>-0.105981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27429</th>\n",
       "      <td>-0.759696</td>\n",
       "      <td>-0.888808</td>\n",
       "      <td>-1.004920</td>\n",
       "      <td>-1.050577</td>\n",
       "      <td>-1.164385</td>\n",
       "      <td>-1.201698</td>\n",
       "      <td>-1.327178</td>\n",
       "      <td>-1.376642</td>\n",
       "      <td>-1.438081</td>\n",
       "      <td>-1.647566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594433</td>\n",
       "      <td>0.480897</td>\n",
       "      <td>0.398505</td>\n",
       "      <td>0.264878</td>\n",
       "      <td>0.220134</td>\n",
       "      <td>0.177535</td>\n",
       "      <td>0.158938</td>\n",
       "      <td>0.111119</td>\n",
       "      <td>-0.316588</td>\n",
       "      <td>-0.416561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27430</th>\n",
       "      <td>0.014039</td>\n",
       "      <td>-0.037562</td>\n",
       "      <td>-0.057552</td>\n",
       "      <td>-0.065982</td>\n",
       "      <td>-0.086522</td>\n",
       "      <td>-0.149793</td>\n",
       "      <td>-0.127718</td>\n",
       "      <td>-0.158629</td>\n",
       "      <td>-0.213004</td>\n",
       "      <td>-0.240857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735610</td>\n",
       "      <td>0.664071</td>\n",
       "      <td>0.553722</td>\n",
       "      <td>0.453239</td>\n",
       "      <td>0.439787</td>\n",
       "      <td>0.398206</td>\n",
       "      <td>0.347891</td>\n",
       "      <td>0.363893</td>\n",
       "      <td>0.375136</td>\n",
       "      <td>0.359889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27431</th>\n",
       "      <td>2.141812</td>\n",
       "      <td>2.115591</td>\n",
       "      <td>2.093230</td>\n",
       "      <td>2.084581</td>\n",
       "      <td>2.096149</td>\n",
       "      <td>2.120108</td>\n",
       "      <td>2.156968</td>\n",
       "      <td>2.217981</td>\n",
       "      <td>2.267777</td>\n",
       "      <td>2.316797</td>\n",
       "      <td>...</td>\n",
       "      <td>1.786589</td>\n",
       "      <td>1.640995</td>\n",
       "      <td>1.578149</td>\n",
       "      <td>1.504923</td>\n",
       "      <td>1.459600</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>1.450113</td>\n",
       "      <td>1.469781</td>\n",
       "      <td>1.475604</td>\n",
       "      <td>1.151867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27432</th>\n",
       "      <td>0.449265</td>\n",
       "      <td>0.438135</td>\n",
       "      <td>0.403330</td>\n",
       "      <td>0.348585</td>\n",
       "      <td>0.317676</td>\n",
       "      <td>0.265433</td>\n",
       "      <td>0.214985</td>\n",
       "      <td>0.168155</td>\n",
       "      <td>0.123892</td>\n",
       "      <td>0.078850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989880</td>\n",
       "      <td>-1.152399</td>\n",
       "      <td>-1.153657</td>\n",
       "      <td>0.971233</td>\n",
       "      <td>1.287016</td>\n",
       "      <td>1.044457</td>\n",
       "      <td>0.961986</td>\n",
       "      <td>1.059022</td>\n",
       "      <td>0.956812</td>\n",
       "      <td>1.074222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27433</th>\n",
       "      <td>0.328369</td>\n",
       "      <td>0.337988</td>\n",
       "      <td>0.403330</td>\n",
       "      <td>0.400406</td>\n",
       "      <td>0.425462</td>\n",
       "      <td>0.459205</td>\n",
       "      <td>0.443453</td>\n",
       "      <td>0.435523</td>\n",
       "      <td>0.460788</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500316</td>\n",
       "      <td>0.068757</td>\n",
       "      <td>-0.470706</td>\n",
       "      <td>-0.253116</td>\n",
       "      <td>0.784954</td>\n",
       "      <td>0.681926</td>\n",
       "      <td>0.725796</td>\n",
       "      <td>0.743055</td>\n",
       "      <td>0.752439</td>\n",
       "      <td>0.779171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27434</th>\n",
       "      <td>0.981208</td>\n",
       "      <td>0.938868</td>\n",
       "      <td>0.915421</td>\n",
       "      <td>0.866793</td>\n",
       "      <td>0.883554</td>\n",
       "      <td>0.874431</td>\n",
       "      <td>0.843273</td>\n",
       "      <td>0.792015</td>\n",
       "      <td>0.797684</td>\n",
       "      <td>0.750234</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033649</td>\n",
       "      <td>0.893037</td>\n",
       "      <td>0.802068</td>\n",
       "      <td>0.720084</td>\n",
       "      <td>0.643749</td>\n",
       "      <td>0.618877</td>\n",
       "      <td>0.599827</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.610950</td>\n",
       "      <td>0.608352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27435</th>\n",
       "      <td>0.860312</td>\n",
       "      <td>0.838721</td>\n",
       "      <td>0.813003</td>\n",
       "      <td>0.737241</td>\n",
       "      <td>0.721874</td>\n",
       "      <td>0.680659</td>\n",
       "      <td>0.643363</td>\n",
       "      <td>0.613769</td>\n",
       "      <td>0.552669</td>\n",
       "      <td>0.462498</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.319291</td>\n",
       "      <td>0.587748</td>\n",
       "      <td>0.786546</td>\n",
       "      <td>0.311968</td>\n",
       "      <td>-0.532959</td>\n",
       "      <td>-1.477498</td>\n",
       "      <td>-1.966776</td>\n",
       "      <td>-2.132253</td>\n",
       "      <td>-2.014454</td>\n",
       "      <td>-1.783113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27436</th>\n",
       "      <td>0.594341</td>\n",
       "      <td>0.538281</td>\n",
       "      <td>0.480144</td>\n",
       "      <td>0.426316</td>\n",
       "      <td>0.371569</td>\n",
       "      <td>0.320796</td>\n",
       "      <td>0.272102</td>\n",
       "      <td>0.227570</td>\n",
       "      <td>0.185146</td>\n",
       "      <td>0.142791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.268943</td>\n",
       "      <td>1.045682</td>\n",
       "      <td>0.988327</td>\n",
       "      <td>0.892749</td>\n",
       "      <td>0.832022</td>\n",
       "      <td>0.808023</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.790450</td>\n",
       "      <td>0.736718</td>\n",
       "      <td>0.220128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27437</th>\n",
       "      <td>0.183294</td>\n",
       "      <td>0.237841</td>\n",
       "      <td>0.249703</td>\n",
       "      <td>0.219033</td>\n",
       "      <td>0.236836</td>\n",
       "      <td>0.320796</td>\n",
       "      <td>0.386336</td>\n",
       "      <td>0.435523</td>\n",
       "      <td>0.460788</td>\n",
       "      <td>0.558410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.817331</td>\n",
       "      <td>-0.923432</td>\n",
       "      <td>-1.091571</td>\n",
       "      <td>-1.273406</td>\n",
       "      <td>-1.427256</td>\n",
       "      <td>-1.414449</td>\n",
       "      <td>-1.494395</td>\n",
       "      <td>0.774651</td>\n",
       "      <td>1.208348</td>\n",
       "      <td>1.105280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27438</th>\n",
       "      <td>-0.590442</td>\n",
       "      <td>-0.638442</td>\n",
       "      <td>-0.646456</td>\n",
       "      <td>-0.687831</td>\n",
       "      <td>-0.733240</td>\n",
       "      <td>-0.758791</td>\n",
       "      <td>-0.841683</td>\n",
       "      <td>-0.901320</td>\n",
       "      <td>-0.948050</td>\n",
       "      <td>-1.008153</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.115370</td>\n",
       "      <td>-1.457688</td>\n",
       "      <td>-2.178085</td>\n",
       "      <td>-0.504264</td>\n",
       "      <td>-0.172101</td>\n",
       "      <td>-0.311094</td>\n",
       "      <td>-0.313443</td>\n",
       "      <td>-0.299640</td>\n",
       "      <td>-0.332309</td>\n",
       "      <td>-0.354445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27439</th>\n",
       "      <td>-0.517904</td>\n",
       "      <td>-0.538295</td>\n",
       "      <td>-0.467224</td>\n",
       "      <td>-0.454638</td>\n",
       "      <td>-0.463774</td>\n",
       "      <td>-0.398928</td>\n",
       "      <td>-0.356187</td>\n",
       "      <td>-0.366583</td>\n",
       "      <td>-0.366139</td>\n",
       "      <td>-0.400710</td>\n",
       "      <td>...</td>\n",
       "      <td>1.065021</td>\n",
       "      <td>0.923566</td>\n",
       "      <td>0.833111</td>\n",
       "      <td>0.720084</td>\n",
       "      <td>0.659439</td>\n",
       "      <td>0.650401</td>\n",
       "      <td>0.631319</td>\n",
       "      <td>0.632466</td>\n",
       "      <td>0.626671</td>\n",
       "      <td>0.608352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27440</th>\n",
       "      <td>0.691058</td>\n",
       "      <td>0.713538</td>\n",
       "      <td>0.710584</td>\n",
       "      <td>0.711330</td>\n",
       "      <td>0.721874</td>\n",
       "      <td>0.708341</td>\n",
       "      <td>0.700480</td>\n",
       "      <td>0.762307</td>\n",
       "      <td>0.889565</td>\n",
       "      <td>0.846146</td>\n",
       "      <td>...</td>\n",
       "      <td>1.253256</td>\n",
       "      <td>1.137268</td>\n",
       "      <td>1.065935</td>\n",
       "      <td>0.971233</td>\n",
       "      <td>0.910470</td>\n",
       "      <td>0.902597</td>\n",
       "      <td>0.899002</td>\n",
       "      <td>0.916837</td>\n",
       "      <td>0.925370</td>\n",
       "      <td>0.934461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27441</th>\n",
       "      <td>-0.179395</td>\n",
       "      <td>-0.212818</td>\n",
       "      <td>-0.236783</td>\n",
       "      <td>-0.195534</td>\n",
       "      <td>-0.140416</td>\n",
       "      <td>-0.066748</td>\n",
       "      <td>-0.042043</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.078850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472233</td>\n",
       "      <td>-0.724994</td>\n",
       "      <td>-0.812182</td>\n",
       "      <td>-0.865290</td>\n",
       "      <td>-0.925194</td>\n",
       "      <td>-0.957345</td>\n",
       "      <td>-0.959030</td>\n",
       "      <td>-0.978971</td>\n",
       "      <td>-0.992590</td>\n",
       "      <td>-0.960076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27442</th>\n",
       "      <td>0.860312</td>\n",
       "      <td>0.838721</td>\n",
       "      <td>0.761794</td>\n",
       "      <td>0.711330</td>\n",
       "      <td>0.667981</td>\n",
       "      <td>0.597614</td>\n",
       "      <td>0.557687</td>\n",
       "      <td>0.524646</td>\n",
       "      <td>0.491415</td>\n",
       "      <td>0.430527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751296</td>\n",
       "      <td>0.618277</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.421846</td>\n",
       "      <td>0.361339</td>\n",
       "      <td>0.335157</td>\n",
       "      <td>0.300653</td>\n",
       "      <td>0.316498</td>\n",
       "      <td>0.327973</td>\n",
       "      <td>0.313302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27443</th>\n",
       "      <td>1.077925</td>\n",
       "      <td>1.064051</td>\n",
       "      <td>1.094653</td>\n",
       "      <td>1.151807</td>\n",
       "      <td>1.126073</td>\n",
       "      <td>1.095885</td>\n",
       "      <td>1.071742</td>\n",
       "      <td>1.089091</td>\n",
       "      <td>1.103954</td>\n",
       "      <td>1.133882</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017962</td>\n",
       "      <td>1.640995</td>\n",
       "      <td>1.578149</td>\n",
       "      <td>1.504923</td>\n",
       "      <td>1.459600</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>1.182431</td>\n",
       "      <td>0.553474</td>\n",
       "      <td>-0.127936</td>\n",
       "      <td>-0.447619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27444</th>\n",
       "      <td>0.062397</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.070471</td>\n",
       "      <td>0.141302</td>\n",
       "      <td>0.155997</td>\n",
       "      <td>0.182388</td>\n",
       "      <td>0.214985</td>\n",
       "      <td>0.227570</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.270674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534978</td>\n",
       "      <td>0.847244</td>\n",
       "      <td>0.910719</td>\n",
       "      <td>0.845659</td>\n",
       "      <td>0.800644</td>\n",
       "      <td>0.792261</td>\n",
       "      <td>0.788780</td>\n",
       "      <td>0.806248</td>\n",
       "      <td>0.831044</td>\n",
       "      <td>0.810229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27445</th>\n",
       "      <td>0.521803</td>\n",
       "      <td>0.513245</td>\n",
       "      <td>0.480144</td>\n",
       "      <td>0.452226</td>\n",
       "      <td>0.398516</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>0.329219</td>\n",
       "      <td>0.286985</td>\n",
       "      <td>0.215773</td>\n",
       "      <td>0.206733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.159139</td>\n",
       "      <td>1.030417</td>\n",
       "      <td>0.895197</td>\n",
       "      <td>0.767175</td>\n",
       "      <td>0.769265</td>\n",
       "      <td>0.603114</td>\n",
       "      <td>0.064462</td>\n",
       "      <td>-0.678801</td>\n",
       "      <td>-1.354172</td>\n",
       "      <td>-1.736526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27446</th>\n",
       "      <td>-0.300291</td>\n",
       "      <td>-0.338002</td>\n",
       "      <td>-0.339202</td>\n",
       "      <td>-0.325086</td>\n",
       "      <td>-0.355988</td>\n",
       "      <td>-0.398928</td>\n",
       "      <td>-0.470421</td>\n",
       "      <td>-0.515121</td>\n",
       "      <td>-0.580527</td>\n",
       "      <td>-0.656475</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.213408</td>\n",
       "      <td>-2.251439</td>\n",
       "      <td>-2.364344</td>\n",
       "      <td>-2.434968</td>\n",
       "      <td>-2.478449</td>\n",
       "      <td>-2.565091</td>\n",
       "      <td>-2.565126</td>\n",
       "      <td>-2.558810</td>\n",
       "      <td>-2.533246</td>\n",
       "      <td>-2.481917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27447</th>\n",
       "      <td>0.594341</td>\n",
       "      <td>0.588355</td>\n",
       "      <td>0.608166</td>\n",
       "      <td>0.607689</td>\n",
       "      <td>0.614088</td>\n",
       "      <td>0.652977</td>\n",
       "      <td>0.700480</td>\n",
       "      <td>0.702892</td>\n",
       "      <td>0.705804</td>\n",
       "      <td>0.718263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.253256</td>\n",
       "      <td>1.228855</td>\n",
       "      <td>1.174587</td>\n",
       "      <td>1.081110</td>\n",
       "      <td>1.051675</td>\n",
       "      <td>1.044457</td>\n",
       "      <td>1.009224</td>\n",
       "      <td>1.043224</td>\n",
       "      <td>1.035417</td>\n",
       "      <td>1.043164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27448</th>\n",
       "      <td>0.086577</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.044866</td>\n",
       "      <td>0.089481</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>0.071661</td>\n",
       "      <td>0.100750</td>\n",
       "      <td>0.079032</td>\n",
       "      <td>0.123892</td>\n",
       "      <td>0.174762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519292</td>\n",
       "      <td>0.267195</td>\n",
       "      <td>0.212246</td>\n",
       "      <td>0.060820</td>\n",
       "      <td>-0.046586</td>\n",
       "      <td>-0.153472</td>\n",
       "      <td>-0.250459</td>\n",
       "      <td>-0.220648</td>\n",
       "      <td>-0.348030</td>\n",
       "      <td>0.561766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27449</th>\n",
       "      <td>0.400907</td>\n",
       "      <td>0.488208</td>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.581778</td>\n",
       "      <td>0.614088</td>\n",
       "      <td>0.680659</td>\n",
       "      <td>0.671922</td>\n",
       "      <td>0.673184</td>\n",
       "      <td>0.736430</td>\n",
       "      <td>0.782205</td>\n",
       "      <td>...</td>\n",
       "      <td>1.472864</td>\n",
       "      <td>1.320442</td>\n",
       "      <td>1.252195</td>\n",
       "      <td>1.159594</td>\n",
       "      <td>1.083053</td>\n",
       "      <td>1.044457</td>\n",
       "      <td>1.024970</td>\n",
       "      <td>1.043224</td>\n",
       "      <td>1.035417</td>\n",
       "      <td>1.027635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27450</th>\n",
       "      <td>1.053746</td>\n",
       "      <td>1.013978</td>\n",
       "      <td>0.992234</td>\n",
       "      <td>0.944524</td>\n",
       "      <td>0.964393</td>\n",
       "      <td>0.957476</td>\n",
       "      <td>0.928949</td>\n",
       "      <td>0.910845</td>\n",
       "      <td>0.889565</td>\n",
       "      <td>0.910087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142821</td>\n",
       "      <td>0.267195</td>\n",
       "      <td>-0.843225</td>\n",
       "      <td>-1.289103</td>\n",
       "      <td>-1.725356</td>\n",
       "      <td>0.587352</td>\n",
       "      <td>1.119446</td>\n",
       "      <td>0.600869</td>\n",
       "      <td>0.956812</td>\n",
       "      <td>1.012106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27451</th>\n",
       "      <td>0.134935</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>0.147285</td>\n",
       "      <td>0.115391</td>\n",
       "      <td>0.102103</td>\n",
       "      <td>0.071661</td>\n",
       "      <td>0.072192</td>\n",
       "      <td>0.049324</td>\n",
       "      <td>0.062638</td>\n",
       "      <td>0.046879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892472</td>\n",
       "      <td>0.770922</td>\n",
       "      <td>0.693416</td>\n",
       "      <td>0.610207</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>0.505351</td>\n",
       "      <td>0.521877</td>\n",
       "      <td>0.532346</td>\n",
       "      <td>0.530708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27452</th>\n",
       "      <td>0.691058</td>\n",
       "      <td>0.638428</td>\n",
       "      <td>0.582562</td>\n",
       "      <td>0.529958</td>\n",
       "      <td>0.479355</td>\n",
       "      <td>0.459205</td>\n",
       "      <td>0.414895</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.277027</td>\n",
       "      <td>0.238703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315370</td>\n",
       "      <td>0.740393</td>\n",
       "      <td>0.864154</td>\n",
       "      <td>0.767175</td>\n",
       "      <td>0.690817</td>\n",
       "      <td>0.650401</td>\n",
       "      <td>0.631319</td>\n",
       "      <td>0.632466</td>\n",
       "      <td>0.610950</td>\n",
       "      <td>0.623881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27453</th>\n",
       "      <td>0.763595</td>\n",
       "      <td>0.813685</td>\n",
       "      <td>0.838607</td>\n",
       "      <td>0.814972</td>\n",
       "      <td>0.829661</td>\n",
       "      <td>0.846749</td>\n",
       "      <td>0.843273</td>\n",
       "      <td>0.851430</td>\n",
       "      <td>0.828311</td>\n",
       "      <td>0.782205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346743</td>\n",
       "      <td>-1.396630</td>\n",
       "      <td>-1.960782</td>\n",
       "      <td>-1.587342</td>\n",
       "      <td>-0.940884</td>\n",
       "      <td>-1.319876</td>\n",
       "      <td>-1.825062</td>\n",
       "      <td>-1.547713</td>\n",
       "      <td>-1.165521</td>\n",
       "      <td>-1.037721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27454</th>\n",
       "      <td>0.811954</td>\n",
       "      <td>0.788648</td>\n",
       "      <td>0.736189</td>\n",
       "      <td>0.685420</td>\n",
       "      <td>0.694928</td>\n",
       "      <td>0.625295</td>\n",
       "      <td>0.614805</td>\n",
       "      <td>0.613769</td>\n",
       "      <td>0.552669</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.519292</td>\n",
       "      <td>-0.236532</td>\n",
       "      <td>0.258811</td>\n",
       "      <td>0.547420</td>\n",
       "      <td>0.816333</td>\n",
       "      <td>0.634639</td>\n",
       "      <td>0.536843</td>\n",
       "      <td>0.679861</td>\n",
       "      <td>0.752439</td>\n",
       "      <td>0.856816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27455 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pixel1    pixel2    pixel3    pixel4    pixel5    pixel6    pixel7  \\\n",
       "0     -0.928951 -0.763625 -0.620852 -0.506458 -0.463774 -0.426610 -0.413304   \n",
       "1      0.231652  0.212805  0.121680  0.063570 -0.005683 -0.039066 -0.127718   \n",
       "2      1.005388  0.988941  0.941025  0.866793  0.829661  0.763704  0.757597   \n",
       "3      1.585689  1.564785  1.555534  1.514553  1.476378  1.428065  1.443003   \n",
       "4      0.449265  0.463171  0.480144  0.478137  0.533248  0.569932  0.557687   \n",
       "5      0.376728  0.488208  0.531353  0.504047  0.587142  0.708341  0.814715   \n",
       "6     -0.276112 -0.363038 -0.416015 -0.480548 -0.544614 -0.592700 -0.670331   \n",
       "7     -0.759696 -2.666411 -1.977892 -1.413322 -1.406904 -1.367789 -1.241503   \n",
       "8      0.570161  0.638428  0.633771  0.685420  0.721874  0.736022  0.757597   \n",
       "9      1.053746  1.013978  0.966630  0.944524  0.910500  0.902113  0.843273   \n",
       "10    -0.300291 -0.338002 -0.262388 -0.195534 -0.167362 -0.094429 -0.070601   \n",
       "11    -3.516129 -3.092035 -2.899656 -2.942036 -3.104537 -3.111737 -3.154927   \n",
       "12    -1.412536 -1.439615 -1.337779 -0.972846 -0.652400 -0.454292 -0.384746   \n",
       "13    -1.581790 -1.264358 -0.774479 -2.967946 -2.781178 -1.783014 -1.726998   \n",
       "14    -0.445366 -0.538295 -0.595247 -0.610100 -0.652400 -0.703427 -0.784566   \n",
       "15    -1.436715 -1.539762 -1.593824 -1.568785 -1.406904 -1.229380 -1.098710   \n",
       "16    -0.662980 -0.713552 -0.595247 -0.480548 -0.463774 -0.371247 -0.327628   \n",
       "17     1.875840  1.915298  1.913998  1.903209  1.934470  1.954018  1.985616   \n",
       "18     1.053746  1.114125  1.120257  1.125897  1.206912  1.317338  1.357327   \n",
       "19     0.666878  0.638428  0.633771  0.607689  0.533248  0.514568  0.472012   \n",
       "20     0.086577  0.037548 -0.031947 -0.091892 -0.167362 -0.205156 -0.270511   \n",
       "21    -0.348650 -0.338002 -0.313597 -0.273265 -0.302095 -0.343565 -0.327628   \n",
       "22    -1.339998 -1.264358 -1.184151 -1.128308 -1.029652 -0.952563 -0.984476   \n",
       "23    -1.460894 -1.464652 -1.388988 -1.309681 -1.002705 -0.786472 -0.641773   \n",
       "24    -0.348650 -0.262892 -0.185574 -0.195534 -0.221255 -0.232838 -0.241953   \n",
       "25     0.183294  0.212805  0.300912  0.322674  0.371569  0.376160  0.414895   \n",
       "26     0.062397  0.112658  0.121680  0.089481  0.048210 -0.011384 -0.070601   \n",
       "27    -0.179395 -0.187782 -0.211179 -0.169623 -0.194309 -0.232838 -0.299070   \n",
       "28    -1.775224 -1.715018 -1.645033 -1.594695 -1.622476 -1.589242 -1.584206   \n",
       "29     0.014039  0.012511 -0.031947 -0.091892 -0.140416 -0.177475 -0.241953   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "27425 -0.735517 -0.663478 -0.646456 -0.610100 -0.571561 -0.565019 -0.584656   \n",
       "27426  0.473444  0.513245  0.505748  0.529958  0.506302  0.514568  0.529129   \n",
       "27427  0.787775  0.738575  0.710584  0.659510  0.587142  0.542250  0.472012   \n",
       "27428 -1.146564 -1.139175 -1.056129 -0.998756 -1.002705 -0.952563 -0.984476   \n",
       "27429 -0.759696 -0.888808 -1.004920 -1.050577 -1.164385 -1.201698 -1.327178   \n",
       "27430  0.014039 -0.037562 -0.057552 -0.065982 -0.086522 -0.149793 -0.127718   \n",
       "27431  2.141812  2.115591  2.093230  2.084581  2.096149  2.120108  2.156968   \n",
       "27432  0.449265  0.438135  0.403330  0.348585  0.317676  0.265433  0.214985   \n",
       "27433  0.328369  0.337988  0.403330  0.400406  0.425462  0.459205  0.443453   \n",
       "27434  0.981208  0.938868  0.915421  0.866793  0.883554  0.874431  0.843273   \n",
       "27435  0.860312  0.838721  0.813003  0.737241  0.721874  0.680659  0.643363   \n",
       "27436  0.594341  0.538281  0.480144  0.426316  0.371569  0.320796  0.272102   \n",
       "27437  0.183294  0.237841  0.249703  0.219033  0.236836  0.320796  0.386336   \n",
       "27438 -0.590442 -0.638442 -0.646456 -0.687831 -0.733240 -0.758791 -0.841683   \n",
       "27439 -0.517904 -0.538295 -0.467224 -0.454638 -0.463774 -0.398928 -0.356187   \n",
       "27440  0.691058  0.713538  0.710584  0.711330  0.721874  0.708341  0.700480   \n",
       "27441 -0.179395 -0.212818 -0.236783 -0.195534 -0.140416 -0.066748 -0.042043   \n",
       "27442  0.860312  0.838721  0.761794  0.711330  0.667981  0.597614  0.557687   \n",
       "27443  1.077925  1.064051  1.094653  1.151807  1.126073  1.095885  1.071742   \n",
       "27444  0.062397  0.062585  0.070471  0.141302  0.155997  0.182388  0.214985   \n",
       "27445  0.521803  0.513245  0.480144  0.452226  0.398516  0.376160  0.329219   \n",
       "27446 -0.300291 -0.338002 -0.339202 -0.325086 -0.355988 -0.398928 -0.470421   \n",
       "27447  0.594341  0.588355  0.608166  0.607689  0.614088  0.652977  0.700480   \n",
       "27448  0.086577  0.062585  0.044866  0.089481  0.075157  0.071661  0.100750   \n",
       "27449  0.400907  0.488208  0.531353  0.581778  0.614088  0.680659  0.671922   \n",
       "27450  1.053746  1.013978  0.992234  0.944524  0.964393  0.957476  0.928949   \n",
       "27451  0.134935  0.137695  0.147285  0.115391  0.102103  0.071661  0.072192   \n",
       "27452  0.691058  0.638428  0.582562  0.529958  0.479355  0.459205  0.414895   \n",
       "27453  0.763595  0.813685  0.838607  0.814972  0.829661  0.846749  0.843273   \n",
       "27454  0.811954  0.788648  0.736189  0.685420  0.694928  0.625295  0.614805   \n",
       "\n",
       "         pixel8    pixel9   pixel10    ...     pixel775  pixel776  pixel777  \\\n",
       "0     -0.366583 -0.335512 -0.304798    ...     1.033649  0.908302  0.833111   \n",
       "1     -0.128922 -0.182377 -0.272827    ...    -1.131056  0.022964 -0.393098   \n",
       "2      0.762307  0.705804  0.654322    ...     0.955217  0.816715  0.724459   \n",
       "3      1.415875  1.410223  1.453589    ...     1.472864  1.320442  1.236673   \n",
       "4      0.643477  0.644550  0.654322    ...    -0.770272 -0.648672 -0.750095   \n",
       "5      0.910845  0.981446  1.165853    ...    -1.021252 -1.121870 -1.324395   \n",
       "6     -0.723074 -0.794916 -0.880270    ...    -0.503605 -0.694465 -0.967398   \n",
       "7     -1.049858 -0.672408 -0.432681    ...     1.143453  1.076211  1.034892   \n",
       "8      0.762307  0.797684  0.814175    ...    -0.346743 -0.450234 -0.470706   \n",
       "9      0.821722  0.797684  0.750234    ...    -2.009487 -1.442423  0.724459   \n",
       "10    -0.099214 -0.029243 -0.017062    ...    -0.660468 -0.786052 -0.889790   \n",
       "11    -3.159100 -2.908173 -2.702598    ...    -0.189880 -0.953961 -1.448568   \n",
       "12    -0.277460 -0.121123 -0.049033    ...     0.547374  0.679335 -0.393098   \n",
       "13    -1.644011 -1.652469 -1.199976    ...     1.457178  1.244120  1.252195   \n",
       "14    -0.871612 -0.886796 -0.944211    ...    -1.444781  0.053493  0.010465   \n",
       "15    -0.931028 -0.825543 -0.816329    ...     1.739531  1.473086  1.314281   \n",
       "16    -0.366583 -0.335512 -0.304798    ...    -0.331056 -1.045548 -0.827703   \n",
       "17     2.010028  2.022761  2.093002    ...     0.578747 -0.267061 -1.060528   \n",
       "18     1.504998  1.563357  1.677383    ...     0.657178  0.877773  0.134638   \n",
       "19     0.435523  0.399534  0.366586    ...     0.955217  0.801451  0.724459   \n",
       "20    -0.336875 -0.396766 -0.464651    ...    -1.491840 -2.190381 -1.308874   \n",
       "21    -0.307167 -0.335512 -0.304798    ...     0.516002  0.679335  0.584765   \n",
       "22    -1.049858 -1.009304 -0.944211    ...    -2.213408 -0.328119  1.205630   \n",
       "23    -0.455706 -0.366139 -0.272827    ...     1.206198  1.106740  1.050414   \n",
       "24    -0.277460 -0.304885 -0.304798    ...    -0.566350  0.572484  0.320897   \n",
       "25     0.435523  0.430161  0.430527    ...     1.049335  0.908302  0.817589   \n",
       "26    -0.128922 -0.121123 -0.176915    ...     0.672865  0.572484  0.476113   \n",
       "27    -0.336875 -0.304885 -0.368739    ...     0.531688  0.419839  0.351940   \n",
       "28    -1.584596 -1.591216 -1.583624    ...    -0.738899 -1.198192 -0.967398   \n",
       "29    -0.277460 -0.335512 -0.368739    ...     0.861100  0.755657  0.677895   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "27425 -0.574536 -0.611154 -0.624505    ...    -1.868311  0.480897  0.460592   \n",
       "27426  0.524646  0.583296  0.558410    ...     0.092473  1.152533  0.926241   \n",
       "27427  0.405816  0.399534  0.334615    ...    -1.648703 -1.686655 -1.914217   \n",
       "27428 -0.960735 -0.978677 -1.008153    ...    -1.287919 -1.198192 -1.122614   \n",
       "27429 -1.376642 -1.438081 -1.647566    ...     0.594433  0.480897  0.398505   \n",
       "27430 -0.158629 -0.213004 -0.240857    ...     0.735610  0.664071  0.553722   \n",
       "27431  2.217981  2.267777  2.316797    ...     1.786589  1.640995  1.578149   \n",
       "27432  0.168155  0.123892  0.078850    ...    -0.989880 -1.152399 -1.153657   \n",
       "27433  0.435523  0.460788  0.526439    ...     0.500316  0.068757 -0.470706   \n",
       "27434  0.792015  0.797684  0.750234    ...     1.033649  0.893037  0.802068   \n",
       "27435  0.613769  0.552669  0.462498    ...    -1.319291  0.587748  0.786546   \n",
       "27436  0.227570  0.185146  0.142791    ...     1.268943  1.045682  0.988327   \n",
       "27437  0.435523  0.460788  0.558410    ...    -0.817331 -0.923432 -1.091571   \n",
       "27438 -0.901320 -0.948050 -1.008153    ...    -1.115370 -1.457688 -2.178085   \n",
       "27439 -0.366583 -0.366139 -0.400710    ...     1.065021  0.923566  0.833111   \n",
       "27440  0.762307  0.889565  0.846146    ...     1.253256  1.137268  1.065935   \n",
       "27441  0.019616  0.001384  0.078850    ...    -0.472233 -0.724994 -0.812182   \n",
       "27442  0.524646  0.491415  0.430527    ...     0.751296  0.618277  0.538200   \n",
       "27443  1.089091  1.103954  1.133882    ...     1.017962  1.640995  1.578149   \n",
       "27444  0.227570  0.246400  0.270674    ...    -0.534978  0.847244  0.910719   \n",
       "27445  0.286985  0.215773  0.206733    ...     1.159139  1.030417  0.895197   \n",
       "27446 -0.515121 -0.580527 -0.656475    ...    -2.213408 -2.251439 -2.364344   \n",
       "27447  0.702892  0.705804  0.718263    ...     1.253256  1.228855  1.174587   \n",
       "27448  0.079032  0.123892  0.174762    ...    -0.519292  0.267195  0.212246   \n",
       "27449  0.673184  0.736430  0.782205    ...     1.472864  1.320442  1.252195   \n",
       "27450  0.910845  0.889565  0.910087    ...    -0.142821  0.267195 -0.843225   \n",
       "27451  0.049324  0.062638  0.046879    ...     0.892472  0.770922  0.693416   \n",
       "27452  0.346400  0.277027  0.238703    ...    -0.315370  0.740393  0.864154   \n",
       "27453  0.851430  0.828311  0.782205    ...    -0.346743 -1.396630 -1.960782   \n",
       "27454  0.613769  0.552669  0.526439    ...    -0.519292 -0.236532  0.258811   \n",
       "\n",
       "       pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0      0.751478  0.690817  0.681926  0.678557  0.664063  0.658113  0.654939  \n",
       "1     -1.132135 -1.066399  0.004150  0.190430 -0.931575 -0.410913 -0.168097  \n",
       "2      0.625904  0.565302  0.571590  0.552589  0.521877  0.516625  0.546237  \n",
       "3      1.128201  1.067364  0.997170  0.977732  0.948434  1.066859  0.049309  \n",
       "4     -0.802503 -0.454511  0.004150 -0.092998  0.016328  0.044995  0.297773  \n",
       "5     -1.524555 -1.709666 -1.698169 -1.809316  1.201208  1.475604  1.477976  \n",
       "6     -1.477464 -0.376064  0.413968  0.253415  0.300699  0.312252  0.297773  \n",
       "7      1.002626  0.957538  0.981408  1.009224  1.027426  1.051138  1.058693  \n",
       "8     -0.614142 -0.689853 -0.768198 -1.084998 -1.389729 -1.574266 -1.596765  \n",
       "9      0.704388  0.612370  0.603114  0.473859  0.205909  0.265089 -0.059394  \n",
       "10    -0.975167 -1.035020 -1.067680 -1.084998 -1.073761 -1.071195 -1.053250  \n",
       "11    -1.493161 -1.411567 -1.461736 -1.510141 -1.531914 -1.558545 -1.488062  \n",
       "12    -2.152426 -2.494138 -2.502042 -2.502142 -2.543012 -2.501804 -2.435330  \n",
       "13     1.190988  1.192879  1.233603  1.245415  1.295998  1.334116  1.338215  \n",
       "14    -0.127542 -0.187791 -0.216521 -0.250459 -0.268043 -0.269425 -0.261271  \n",
       "15     1.332259  1.051675 -0.090423 -1.226713 -0.631406 -0.316588 -0.494206  \n",
       "16    -0.174632 -0.187791 -0.137710 -0.045760  0.063724  0.139321  0.313302  \n",
       "17    -0.975167  1.318395  1.343939  1.213923  0.995829  1.349837  1.477976  \n",
       "18    -0.457174  0.471165  1.107505 -0.754332 -0.789390  0.610950  0.530708  \n",
       "19     0.641601  0.580991  0.540066  0.505351  0.521877  0.500904  0.468592  \n",
       "20     0.374755  0.078929  0.067199  0.064462  0.032127  0.013553  0.018251  \n",
       "21     0.500330  0.455476  0.445492  0.473859  0.458683  0.453741  0.484121  \n",
       "22     0.767175  0.800644  0.808023  0.820272  0.822047  0.831044  0.856816  \n",
       "23     1.002626  1.004606  1.028694  1.040716  1.074821  1.082580  1.089751  \n",
       "24     0.249181  0.173066  0.130248  0.111700  0.079522  0.060716  0.049309  \n",
       "25     0.735781  0.675128  0.650401  0.647065  0.648264  0.642392  0.623881  \n",
       "26     0.343362  0.298582  0.256346  0.190430  0.174312  0.155042  0.111425  \n",
       "27     0.280575  0.251513  0.240583  0.221922  0.237506  0.265089  0.282244  \n",
       "28     0.107910 -0.046586 -0.058899 -0.077252 -0.078462 -0.080773 -0.090452  \n",
       "29     0.578813  0.533923  0.508541  0.489605  0.506079  0.453741  0.422005  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "27425  0.311968  0.267203  0.224821  0.206176  0.205909  0.170763  0.142483  \n",
       "27426  0.845659  0.800644  0.792261  0.773034  0.758853  0.768160  0.748113  \n",
       "27427  0.013729  0.518234  0.398206  0.379383  0.395490  0.390857  0.390947  \n",
       "27428 -1.242013 -1.301741 -1.225303 -0.911792 -0.726196 -0.410913 -0.105981  \n",
       "27429  0.264878  0.220134  0.177535  0.158938  0.111119 -0.316588 -0.416561  \n",
       "27430  0.453239  0.439787  0.398206  0.347891  0.363893  0.375136  0.359889  \n",
       "27431  1.504923  1.459600  1.454274  1.450113  1.469781  1.475604  1.151867  \n",
       "27432  0.971233  1.287016  1.044457  0.961986  1.059022  0.956812  1.074222  \n",
       "27433 -0.253116  0.784954  0.681926  0.725796  0.743055  0.752439  0.779171  \n",
       "27434  0.720084  0.643749  0.618877  0.599827  0.616667  0.610950  0.608352  \n",
       "27435  0.311968 -0.532959 -1.477498 -1.966776 -2.132253 -2.014454 -1.783113  \n",
       "27436  0.892749  0.832022  0.808023  0.773034  0.790450  0.736718  0.220128  \n",
       "27437 -1.273406 -1.427256 -1.414449 -1.494395  0.774651  1.208348  1.105280  \n",
       "27438 -0.504264 -0.172101 -0.311094 -0.313443 -0.299640 -0.332309 -0.354445  \n",
       "27439  0.720084  0.659439  0.650401  0.631319  0.632466  0.626671  0.608352  \n",
       "27440  0.971233  0.910470  0.902597  0.899002  0.916837  0.925370  0.934461  \n",
       "27441 -0.865290 -0.925194 -0.957345 -0.959030 -0.978971 -0.992590 -0.960076  \n",
       "27442  0.421846  0.361339  0.335157  0.300653  0.316498  0.327973  0.313302  \n",
       "27443  1.504923  1.459600  1.454274  1.182431  0.553474 -0.127936 -0.447619  \n",
       "27444  0.845659  0.800644  0.792261  0.788780  0.806248  0.831044  0.810229  \n",
       "27445  0.767175  0.769265  0.603114  0.064462 -0.678801 -1.354172 -1.736526  \n",
       "27446 -2.434968 -2.478449 -2.565091 -2.565126 -2.558810 -2.533246 -2.481917  \n",
       "27447  1.081110  1.051675  1.044457  1.009224  1.043224  1.035417  1.043164  \n",
       "27448  0.060820 -0.046586 -0.153472 -0.250459 -0.220648 -0.348030  0.561766  \n",
       "27449  1.159594  1.083053  1.044457  1.024970  1.043224  1.035417  1.027635  \n",
       "27450 -1.289103 -1.725356  0.587352  1.119446  0.600869  0.956812  1.012106  \n",
       "27451  0.610207  0.565302  0.524303  0.505351  0.521877  0.532346  0.530708  \n",
       "27452  0.767175  0.690817  0.650401  0.631319  0.632466  0.610950  0.623881  \n",
       "27453 -1.587342 -0.940884 -1.319876 -1.825062 -1.547713 -1.165521 -1.037721  \n",
       "27454  0.547420  0.816333  0.634639  0.536843  0.679861  0.752439  0.856816  \n",
       "\n",
       "[27455 rows x 784 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "a=x_tr/255\n",
    "x_tr_scaled = pd.DataFrame(scaler.fit_transform(a), columns=a.columns)\n",
    "x_t_scaled = pd.DataFrame(scaler.transform(x_t), columns=x_t.columns)\n",
    "x_v_scaled = pd.DataFrame(scaler.transform(x_v), columns=x_v.columns)\n",
    "x_tr_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c)\n",
    "\n",
    "Se entrena una red neuronal para la representación original de las imagenes, variando distintos parametros para encontrar una acuaracy mayor que 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27455 samples, validate on 6864 samples\n",
      "Epoch 1/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 15.3604 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 2/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 3/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 4/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 5/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 6/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 7/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 8/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 9/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 10/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 11/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 12/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 13/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 14/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 15/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 16/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 17/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 18/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 19/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 20/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 21/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 22/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 23/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 24/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 25/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 26/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 27/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 28/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 29/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 30/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 31/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 32/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 33/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 34/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 35/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 36/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 37/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 38/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 39/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 40/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 41/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 42/100\n",
      "27455/27455 [==============================] - 1s 38us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 43/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 44/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 45/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 46/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 47/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 48/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 49/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 50/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 51/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 52/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 53/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 54/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 55/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 56/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 57/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 58/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 60/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 61/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 62/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 63/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 64/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 65/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 66/100\n",
      "27455/27455 [==============================] - 1s 38us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 67/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 68/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 69/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 70/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 71/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 72/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 73/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 74/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 75/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 76/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 77/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 78/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 79/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 80/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 81/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 82/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 83/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 84/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 85/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 86/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 87/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 88/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 89/100\n",
      "27455/27455 [==============================] - 1s 31us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 90/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 91/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 92/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 93/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 94/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 95/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 96/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 97/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 98/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 99/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n",
      "Epoch 100/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 15.4113 - acc: 0.0439 - val_loss: 15.3737 - val_acc: 0.0462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210b45317f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrega una capa a la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27455 samples, validate on 6864 samples\n",
      "Epoch 1/100\n",
      "27455/27455 [==============================] - 1s 45us/step - loss: 3.1889 - acc: 0.0484 - val_loss: 3.1686 - val_acc: 0.0699\n",
      "Epoch 2/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1844 - acc: 0.0496 - val_loss: 3.1661 - val_acc: 0.0536\n",
      "Epoch 3/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1917 - acc: 0.0501 - val_loss: 3.1970 - val_acc: 0.0510\n",
      "Epoch 4/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1840 - acc: 0.0473 - val_loss: 3.1918 - val_acc: 0.0510\n",
      "Epoch 5/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1761 - acc: 0.0484 - val_loss: 3.1704 - val_acc: 0.0580\n",
      "Epoch 6/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1691 - acc: 0.0490 - val_loss: 3.1864 - val_acc: 0.0510\n",
      "Epoch 7/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1869 - acc: 0.0464 - val_loss: 3.1834 - val_acc: 0.0510\n",
      "Epoch 8/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1839 - acc: 0.0471 - val_loss: 3.1818 - val_acc: 0.0510\n",
      "Epoch 9/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1833 - acc: 0.0464 - val_loss: 3.1805 - val_acc: 0.0510\n",
      "Epoch 10/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1822 - acc: 0.0460 - val_loss: 3.1795 - val_acc: 0.0485\n",
      "Epoch 11/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1813 - acc: 0.0464 - val_loss: 3.1788 - val_acc: 0.0485\n",
      "Epoch 12/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1807 - acc: 0.0471 - val_loss: 3.1782 - val_acc: 0.0485\n",
      "Epoch 13/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1801 - acc: 0.0468 - val_loss: 3.1777 - val_acc: 0.0485\n",
      "Epoch 14/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1797 - acc: 0.0465 - val_loss: 3.1773 - val_acc: 0.0485\n",
      "Epoch 15/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1793 - acc: 0.0469 - val_loss: 3.1769 - val_acc: 0.0485\n",
      "Epoch 16/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1789 - acc: 0.0469 - val_loss: 3.1766 - val_acc: 0.0485\n",
      "Epoch 17/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1787 - acc: 0.0460 - val_loss: 3.1763 - val_acc: 0.0485\n",
      "Epoch 18/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1784 - acc: 0.0469 - val_loss: 3.1761 - val_acc: 0.0485\n",
      "Epoch 19/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1782 - acc: 0.0470 - val_loss: 3.1759 - val_acc: 0.0485\n",
      "Epoch 20/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1780 - acc: 0.0468 - val_loss: 3.1758 - val_acc: 0.0485\n",
      "Epoch 21/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1779 - acc: 0.0467 - val_loss: 3.1755 - val_acc: 0.0485\n",
      "Epoch 22/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1777 - acc: 0.0465 - val_loss: 3.1754 - val_acc: 0.0485\n",
      "Epoch 23/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 3.1776 - acc: 0.0471 - val_loss: 3.1752 - val_acc: 0.0510\n",
      "Epoch 24/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1775 - acc: 0.0459 - val_loss: 3.1752 - val_acc: 0.0485\n",
      "Epoch 25/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1774 - acc: 0.0475 - val_loss: 3.1750 - val_acc: 0.0510\n",
      "Epoch 26/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1773 - acc: 0.0455 - val_loss: 3.1750 - val_acc: 0.0485\n",
      "Epoch 27/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1773 - acc: 0.0467 - val_loss: 3.1749 - val_acc: 0.0485\n",
      "Epoch 28/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1771 - acc: 0.0467 - val_loss: 3.1748 - val_acc: 0.0485\n",
      "Epoch 29/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1771 - acc: 0.0470 - val_loss: 3.1749 - val_acc: 0.0485\n",
      "Epoch 30/100\n",
      "27455/27455 [==============================] - 1s 38us/step - loss: 3.1770 - acc: 0.0468 - val_loss: 3.1747 - val_acc: 0.0485\n",
      "Epoch 31/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1769 - acc: 0.0461 - val_loss: 3.1747 - val_acc: 0.0485\n",
      "Epoch 32/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1769 - acc: 0.0471 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 33/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1769 - acc: 0.0465 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 34/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1768 - acc: 0.0458 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 35/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1768 - acc: 0.0466 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 36/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1768 - acc: 0.0463 - val_loss: 3.1743 - val_acc: 0.0485\n",
      "Epoch 37/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1767 - acc: 0.0468 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 38/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1767 - acc: 0.0457 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 39/100\n",
      "27455/27455 [==============================] - 1s 38us/step - loss: 3.1767 - acc: 0.0468 - val_loss: 3.1744 - val_acc: 0.0510\n",
      "Epoch 40/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1766 - acc: 0.0469 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 41/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1766 - acc: 0.0463 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 42/100\n",
      "27455/27455 [==============================] - 1s 38us/step - loss: 3.1766 - acc: 0.0467 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 43/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1765 - acc: 0.0460 - val_loss: 3.1743 - val_acc: 0.0485\n",
      "Epoch 44/100\n",
      "27455/27455 [==============================] - 1s 38us/step - loss: 3.1765 - acc: 0.0460 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 45/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1765 - acc: 0.0468 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 46/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1765 - acc: 0.0460 - val_loss: 3.1742 - val_acc: 0.0485\n",
      "Epoch 47/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1764 - acc: 0.0466 - val_loss: 3.1743 - val_acc: 0.0485\n",
      "Epoch 48/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1764 - acc: 0.0469 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 49/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1764 - acc: 0.0460 - val_loss: 3.1737 - val_acc: 0.0485\n",
      "Epoch 50/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0484 - val_loss: 3.1739 - val_acc: 0.0487\n",
      "Epoch 51/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1600 - acc: 0.0482 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 52/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1658 - acc: 0.0483 - val_loss: 3.1297 - val_acc: 0.0457\n",
      "Epoch 53/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1181 - acc: 0.0582 - val_loss: 3.1756 - val_acc: 0.0492\n",
      "Epoch 54/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1237 - acc: 0.0578 - val_loss: 3.1795 - val_acc: 0.0420\n",
      "Epoch 55/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1784 - acc: 0.0459 - val_loss: 3.1735 - val_acc: 0.0511\n",
      "Epoch 56/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1132 - acc: 0.0618 - val_loss: 3.1790 - val_acc: 0.0421\n",
      "Epoch 57/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.0539 - acc: 0.0723 - val_loss: 2.9725 - val_acc: 0.0810\n",
      "Epoch 58/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.0554 - acc: 0.0701 - val_loss: 3.1985 - val_acc: 0.0420\n",
      "Epoch 59/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1890 - acc: 0.0455 - val_loss: 3.1760 - val_acc: 0.0510\n",
      "Epoch 60/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1773 - acc: 0.0465 - val_loss: 3.1740 - val_acc: 0.0510\n",
      "Epoch 61/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1752 - acc: 0.0457 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 62/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1764 - acc: 0.0451 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 63/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 3.1763 - acc: 0.0465 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 64/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1764 - acc: 0.0466 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 65/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1764 - acc: 0.0456 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 66/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0460 - val_loss: 3.1742 - val_acc: 0.0510\n",
      "Epoch 67/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1763 - acc: 0.0467 - val_loss: 3.1740 - val_acc: 0.0510\n",
      "Epoch 68/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1764 - acc: 0.0452 - val_loss: 3.1742 - val_acc: 0.0485\n",
      "Epoch 69/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1763 - acc: 0.0453 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 70/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0456 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 71/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 3.1763 - acc: 0.0465 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 72/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1763 - acc: 0.0459 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 73/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 3.1763 - acc: 0.0454 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 74/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 3.1763 - acc: 0.0459 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 75/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1763 - acc: 0.0465 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 76/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 3.1763 - acc: 0.0459 - val_loss: 3.1738 - val_acc: 0.0510\n",
      "Epoch 77/100\n",
      "27455/27455 [==============================] - 1s 32us/step - loss: 3.1763 - acc: 0.0447 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 78/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0465 - val_loss: 3.1739 - val_acc: 0.0510\n",
      "Epoch 79/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1762 - acc: 0.0476 - val_loss: 3.1739 - val_acc: 0.0510\n",
      "Epoch 80/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1763 - acc: 0.0461 - val_loss: 3.1741 - val_acc: 0.0510\n",
      "Epoch 81/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0457 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 82/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0464 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 83/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 3.1762 - acc: 0.0457 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 84/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1763 - acc: 0.0470 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 85/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1762 - acc: 0.0462 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 86/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1762 - acc: 0.0469 - val_loss: 3.1740 - val_acc: 0.0510\n",
      "Epoch 87/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1763 - acc: 0.0458 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 88/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1762 - acc: 0.0475 - val_loss: 3.1738 - val_acc: 0.0510\n",
      "Epoch 89/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1763 - acc: 0.0461 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 90/100\n",
      "27455/27455 [==============================] - 1s 36us/step - loss: 3.1763 - acc: 0.0456 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 91/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1762 - acc: 0.0466 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 92/100\n",
      "27455/27455 [==============================] - 1s 37us/step - loss: 3.1762 - acc: 0.0469 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 93/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1763 - acc: 0.0465 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 94/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1762 - acc: 0.0468 - val_loss: 3.1739 - val_acc: 0.0485\n",
      "Epoch 95/100\n",
      "27455/27455 [==============================] - 1s 33us/step - loss: 3.1762 - acc: 0.0473 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 96/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1762 - acc: 0.0470 - val_loss: 3.1736 - val_acc: 0.0485\n",
      "Epoch 97/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1762 - acc: 0.0448 - val_loss: 3.1737 - val_acc: 0.0485\n",
      "Epoch 98/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1762 - acc: 0.0449 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 99/100\n",
      "27455/27455 [==============================] - 1s 35us/step - loss: 3.1762 - acc: 0.0463 - val_loss: 3.1737 - val_acc: 0.0485\n",
      "Epoch 100/100\n",
      "27455/27455 [==============================] - 1s 34us/step - loss: 3.1762 - acc: 0.0459 - val_loss: 3.1740 - val_acc: 0.0485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca25c87358>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen levemente mejores resultados que al comienzo pero con bastantes problemas aun, por lo que se procede a cambiar la activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27455 samples, validate on 6864 samples\n",
      "Epoch 1/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 3.2108 - acc: 0.0437 - val_loss: 3.2035 - val_acc: 0.0510\n",
      "Epoch 2/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.2005 - acc: 0.0447 - val_loss: 3.1958 - val_acc: 0.0485\n",
      "Epoch 3/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1948 - acc: 0.0457 - val_loss: 3.1911 - val_acc: 0.0485\n",
      "Epoch 4/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1913 - acc: 0.0462 - val_loss: 3.1881 - val_acc: 0.0485\n",
      "Epoch 5/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1888 - acc: 0.0471 - val_loss: 3.1859 - val_acc: 0.0485\n",
      "Epoch 6/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1871 - acc: 0.0469 - val_loss: 3.1844 - val_acc: 0.0485\n",
      "Epoch 7/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1857 - acc: 0.0462 - val_loss: 3.1831 - val_acc: 0.0485\n",
      "Epoch 8/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1846 - acc: 0.0471 - val_loss: 3.1821 - val_acc: 0.0485\n",
      "Epoch 9/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1838 - acc: 0.0471 - val_loss: 3.1813 - val_acc: 0.0485\n",
      "Epoch 10/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1831 - acc: 0.0471 - val_loss: 3.1807 - val_acc: 0.0485\n",
      "Epoch 11/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1825 - acc: 0.0471 - val_loss: 3.1801 - val_acc: 0.0485\n",
      "Epoch 12/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1820 - acc: 0.0471 - val_loss: 3.1796 - val_acc: 0.0485\n",
      "Epoch 13/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1815 - acc: 0.0469 - val_loss: 3.1792 - val_acc: 0.0485\n",
      "Epoch 14/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1811 - acc: 0.0468 - val_loss: 3.1789 - val_acc: 0.0485\n",
      "Epoch 15/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1808 - acc: 0.0471 - val_loss: 3.1786 - val_acc: 0.0485\n",
      "Epoch 16/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1805 - acc: 0.0469 - val_loss: 3.1783 - val_acc: 0.0485\n",
      "Epoch 17/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1802 - acc: 0.0470 - val_loss: 3.1780 - val_acc: 0.0485\n",
      "Epoch 18/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1800 - acc: 0.0471 - val_loss: 3.1777 - val_acc: 0.0485\n",
      "Epoch 19/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1797 - acc: 0.0469 - val_loss: 3.1775 - val_acc: 0.0485\n",
      "Epoch 20/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1795 - acc: 0.0461 - val_loss: 3.1773 - val_acc: 0.0485\n",
      "Epoch 21/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1794 - acc: 0.0456 - val_loss: 3.1772 - val_acc: 0.0485\n",
      "Epoch 22/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1792 - acc: 0.0471 - val_loss: 3.1770 - val_acc: 0.0485\n",
      "Epoch 23/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1790 - acc: 0.0471 - val_loss: 3.1768 - val_acc: 0.0485\n",
      "Epoch 24/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1789 - acc: 0.0471 - val_loss: 3.1767 - val_acc: 0.0485\n",
      "Epoch 25/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1787 - acc: 0.0466 - val_loss: 3.1766 - val_acc: 0.0485\n",
      "Epoch 26/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1786 - acc: 0.0471 - val_loss: 3.1765 - val_acc: 0.0485\n",
      "Epoch 27/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1785 - acc: 0.0471 - val_loss: 3.1763 - val_acc: 0.0485\n",
      "Epoch 28/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1784 - acc: 0.0471 - val_loss: 3.1762 - val_acc: 0.0485\n",
      "Epoch 29/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1783 - acc: 0.0463 - val_loss: 3.1761 - val_acc: 0.0485\n",
      "Epoch 30/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1781 - acc: 0.0470 - val_loss: 3.1760 - val_acc: 0.0485\n",
      "Epoch 31/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1781 - acc: 0.0469 - val_loss: 3.1759 - val_acc: 0.0485\n",
      "Epoch 32/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1780 - acc: 0.0471 - val_loss: 3.1758 - val_acc: 0.0485\n",
      "Epoch 33/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1779 - acc: 0.0467 - val_loss: 3.1757 - val_acc: 0.0485\n",
      "Epoch 34/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1778 - acc: 0.0471 - val_loss: 3.1756 - val_acc: 0.0485\n",
      "Epoch 35/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1777 - acc: 0.0471 - val_loss: 3.1755 - val_acc: 0.0485\n",
      "Epoch 36/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1776 - acc: 0.0469 - val_loss: 3.1754 - val_acc: 0.0485\n",
      "Epoch 37/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1775 - acc: 0.0471 - val_loss: 3.1754 - val_acc: 0.0485\n",
      "Epoch 38/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1774 - acc: 0.0467 - val_loss: 3.1753 - val_acc: 0.0485\n",
      "Epoch 39/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1773 - acc: 0.0460 - val_loss: 3.1752 - val_acc: 0.0485\n",
      "Epoch 40/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1772 - acc: 0.0467 - val_loss: 3.1751 - val_acc: 0.0485\n",
      "Epoch 41/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1772 - acc: 0.0471 - val_loss: 3.1750 - val_acc: 0.0485\n",
      "Epoch 42/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1771 - acc: 0.0471 - val_loss: 3.1749 - val_acc: 0.0485\n",
      "Epoch 43/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1770 - acc: 0.0465 - val_loss: 3.1748 - val_acc: 0.0485\n",
      "Epoch 44/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1769 - acc: 0.0460 - val_loss: 3.1748 - val_acc: 0.0485\n",
      "Epoch 45/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1768 - acc: 0.0471 - val_loss: 3.1746 - val_acc: 0.0485\n",
      "Epoch 46/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1767 - acc: 0.0471 - val_loss: 3.1746 - val_acc: 0.0485\n",
      "Epoch 47/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1766 - acc: 0.0471 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 48/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1765 - acc: 0.0463 - val_loss: 3.1743 - val_acc: 0.0485\n",
      "Epoch 49/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1764 - acc: 0.0450 - val_loss: 3.1742 - val_acc: 0.0485\n",
      "Epoch 50/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1762 - acc: 0.0466 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 51/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1761 - acc: 0.0471 - val_loss: 3.1740 - val_acc: 0.0485\n",
      "Epoch 52/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1760 - acc: 0.0471 - val_loss: 3.1738 - val_acc: 0.0485\n",
      "Epoch 53/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1758 - acc: 0.0471 - val_loss: 3.1736 - val_acc: 0.0485\n",
      "Epoch 54/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1757 - acc: 0.0471 - val_loss: 3.1735 - val_acc: 0.0485\n",
      "Epoch 55/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1755 - acc: 0.0468 - val_loss: 3.1733 - val_acc: 0.0485\n",
      "Epoch 56/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1753 - acc: 0.0457 - val_loss: 3.1732 - val_acc: 0.0485\n",
      "Epoch 57/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1751 - acc: 0.0466 - val_loss: 3.1730 - val_acc: 0.0485\n",
      "Epoch 58/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1749 - acc: 0.0459 - val_loss: 3.1727 - val_acc: 0.0485\n",
      "Epoch 59/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1747 - acc: 0.0471 - val_loss: 3.1725 - val_acc: 0.0485\n",
      "Epoch 60/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1744 - acc: 0.0460 - val_loss: 3.1722 - val_acc: 0.0485\n",
      "Epoch 61/100\n",
      "27455/27455 [==============================] - 1s 39us/step - loss: 3.1742 - acc: 0.0471 - val_loss: 3.1719 - val_acc: 0.0485\n",
      "Epoch 62/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1738 - acc: 0.0471 - val_loss: 3.1716 - val_acc: 0.0485\n",
      "Epoch 63/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1735 - acc: 0.0471 - val_loss: 3.1713 - val_acc: 0.0485\n",
      "Epoch 64/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1732 - acc: 0.0471 - val_loss: 3.1709 - val_acc: 0.0485\n",
      "Epoch 65/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1728 - acc: 0.0471 - val_loss: 3.1705 - val_acc: 0.0485\n",
      "Epoch 66/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1724 - acc: 0.0471 - val_loss: 3.1701 - val_acc: 0.0485\n",
      "Epoch 67/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1719 - acc: 0.0471 - val_loss: 3.1695 - val_acc: 0.0485\n",
      "Epoch 68/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1715 - acc: 0.0471 - val_loss: 3.1690 - val_acc: 0.0485\n",
      "Epoch 69/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1710 - acc: 0.0471 - val_loss: 3.1690 - val_acc: 0.0485\n",
      "Epoch 70/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1705 - acc: 0.0464 - val_loss: 3.1682 - val_acc: 0.0485\n",
      "Epoch 71/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1698 - acc: 0.0468 - val_loss: 3.1681 - val_acc: 0.0485\n",
      "Epoch 72/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1692 - acc: 0.0471 - val_loss: 3.1663 - val_acc: 0.0485\n",
      "Epoch 73/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1683 - acc: 0.0471 - val_loss: 3.1655 - val_acc: 0.0485\n",
      "Epoch 74/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1673 - acc: 0.0480 - val_loss: 3.1644 - val_acc: 0.0485\n",
      "Epoch 75/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1666 - acc: 0.0566 - val_loss: 3.1636 - val_acc: 0.0947\n",
      "Epoch 76/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1655 - acc: 0.0640 - val_loss: 3.1632 - val_acc: 0.0845\n",
      "Epoch 77/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1642 - acc: 0.0776 - val_loss: 3.1609 - val_acc: 0.0940\n",
      "Epoch 78/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1628 - acc: 0.0875 - val_loss: 3.1617 - val_acc: 0.0874\n",
      "Epoch 79/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1610 - acc: 0.0877 - val_loss: 3.1576 - val_acc: 0.0953\n",
      "Epoch 80/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1595 - acc: 0.0865 - val_loss: 3.1556 - val_acc: 0.0946\n",
      "Epoch 81/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1577 - acc: 0.0865 - val_loss: 3.1530 - val_acc: 0.0953\n",
      "Epoch 82/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1553 - acc: 0.0863 - val_loss: 3.1700 - val_acc: 0.0516\n",
      "Epoch 83/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1534 - acc: 0.0828 - val_loss: 3.1476 - val_acc: 0.0946\n",
      "Epoch 84/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1481 - acc: 0.0858 - val_loss: 3.1429 - val_acc: 0.0905\n",
      "Epoch 85/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1448 - acc: 0.0843 - val_loss: 3.1376 - val_acc: 0.0953\n",
      "Epoch 86/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1376 - acc: 0.0846 - val_loss: 3.1495 - val_acc: 0.0695\n",
      "Epoch 87/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1292 - acc: 0.0808 - val_loss: 3.1204 - val_acc: 0.0906\n",
      "Epoch 88/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1177 - acc: 0.0796 - val_loss: 3.0912 - val_acc: 0.0944\n",
      "Epoch 89/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1125 - acc: 0.0754 - val_loss: 3.1916 - val_acc: 0.0506\n",
      "Epoch 90/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1675 - acc: 0.0511 - val_loss: 3.1750 - val_acc: 0.0485\n",
      "Epoch 91/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1770 - acc: 0.0471 - val_loss: 3.1745 - val_acc: 0.0485\n",
      "Epoch 92/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1766 - acc: 0.0471 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 93/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1765 - acc: 0.0469 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 94/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1765 - acc: 0.0472 - val_loss: 3.1744 - val_acc: 0.0485\n",
      "Epoch 95/100\n",
      "27455/27455 [==============================] - 1s 43us/step - loss: 3.1765 - acc: 0.0472 - val_loss: 3.1742 - val_acc: 0.0485\n",
      "Epoch 96/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1763 - acc: 0.0472 - val_loss: 3.1741 - val_acc: 0.0485\n",
      "Epoch 97/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1298 - acc: 0.0657 - val_loss: 3.0927 - val_acc: 0.0892\n",
      "Epoch 98/100\n",
      "27455/27455 [==============================] - 1s 40us/step - loss: 3.1634 - acc: 0.0571 - val_loss: 3.1881 - val_acc: 0.0492\n",
      "Epoch 99/100\n",
      "27455/27455 [==============================] - 1s 41us/step - loss: 3.1825 - acc: 0.0453 - val_loss: 3.1769 - val_acc: 0.0492\n",
      "Epoch 100/100\n",
      "27455/27455 [==============================] - 1s 42us/step - loss: 3.1775 - acc: 0.0453 - val_loss: 3.1749 - val_acc: 0.0492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca30c1b4e0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='softmax'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba aumentando la cantidad de neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27455 samples, validate on 6864 samples\n",
      "Epoch 1/100\n",
      "27455/27455 [==============================] - 2s 69us/step - loss: 15.3391 - acc: 0.0465 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 2/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 3/100\n",
      "27455/27455 [==============================] - 1s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 4/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 5/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 6/100\n",
      "27455/27455 [==============================] - 2s 58us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 7/100\n",
      "27455/27455 [==============================] - 2s 62us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 8/100\n",
      "27455/27455 [==============================] - 2s 70us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 9/100\n",
      "27455/27455 [==============================] - 2s 68us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 10/100\n",
      "27455/27455 [==============================] - 2s 71us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 11/100\n",
      "27455/27455 [==============================] - 2s 66us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 12/100\n",
      "27455/27455 [==============================] - 2s 65us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 13/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 14/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 15/100\n",
      "27455/27455 [==============================] - 2s 61us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 16/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 17/100\n",
      "27455/27455 [==============================] - 2s 58us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 18/100\n",
      "27455/27455 [==============================] - 2s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 19/100\n",
      "27455/27455 [==============================] - 2s 59us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 20/100\n",
      "27455/27455 [==============================] - 2s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 21/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 22/100\n",
      "27455/27455 [==============================] - 2s 59us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 23/100\n",
      "27455/27455 [==============================] - 2s 61us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 24/100\n",
      "27455/27455 [==============================] - 2s 58us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 25/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 26/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 27/100\n",
      "27455/27455 [==============================] - 2s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 28/100\n",
      "27455/27455 [==============================] - 1s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 29/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 30/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 31/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 32/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 33/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 34/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 35/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 36/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 37/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 38/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 39/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 40/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 41/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 42/100\n",
      "27455/27455 [==============================] - 1s 51us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 43/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 44/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 45/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 46/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 47/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 48/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 49/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 50/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 51/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 52/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 53/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 54/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 55/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 56/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 57/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 58/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 60/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 61/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 62/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 63/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 64/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 65/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 66/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 67/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 68/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 69/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 70/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 71/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 72/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 73/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 74/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 75/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 76/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 77/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 78/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 79/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 80/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 81/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 82/100\n",
      "27455/27455 [==============================] - 1s 52us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 83/100\n",
      "27455/27455 [==============================] - 1s 53us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 84/100\n",
      "27455/27455 [==============================] - 2s 59us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 85/100\n",
      "27455/27455 [==============================] - 2s 60us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 86/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 87/100\n",
      "27455/27455 [==============================] - 2s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 88/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 89/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 90/100\n",
      "27455/27455 [==============================] - 2s 55us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 91/100\n",
      "27455/27455 [==============================] - 1s 54us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 92/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 93/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 94/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 95/100\n",
      "27455/27455 [==============================] - 2s 61us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 96/100\n",
      "27455/27455 [==============================] - 2s 61us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 97/100\n",
      "27455/27455 [==============================] - 2s 60us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 98/100\n",
      "27455/27455 [==============================] - 2s 57us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 99/100\n",
      "27455/27455 [==============================] - 2s 56us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n",
      "Epoch 100/100\n",
      "27455/27455 [==============================] - 2s 59us/step - loss: 15.3672 - acc: 0.0466 - val_loss: 15.2962 - val_acc: 0.0510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210baa29be0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logrando mejorar en una pequeña cantidad el accuracy.\n",
    "Se realiza ahora aumentando la cantidad de neuronas y de capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(250, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(150, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27455 samples, validate on 6864 samples\n",
      "Epoch 1/100\n",
      "27455/27455 [==============================] - 4s 128us/step - loss: 3.2102 - acc: 0.0471 - val_loss: 3.2004 - val_acc: 0.0485\n",
      "Epoch 2/100\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 3.1765 - acc: 0.0526 - val_loss: 3.1771 - val_acc: 0.0495\n",
      "Epoch 3/100\n",
      "27455/27455 [==============================] - 3s 101us/step - loss: 3.1621 - acc: 0.0557 - val_loss: 3.1870 - val_acc: 0.0524\n",
      "Epoch 4/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.1585 - acc: 0.0567 - val_loss: 2.9331 - val_acc: 0.0909\n",
      "Epoch 5/100\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 3.1514 - acc: 0.0595 - val_loss: 3.1747 - val_acc: 0.0720\n",
      "Epoch 6/100\n",
      "27455/27455 [==============================] - 3s 100us/step - loss: 3.1011 - acc: 0.0625 - val_loss: 2.9535 - val_acc: 0.0979\n",
      "Epoch 7/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 2.9671 - acc: 0.0914 - val_loss: 3.1253 - val_acc: 0.0743\n",
      "Epoch 8/100\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 3.0125 - acc: 0.0836 - val_loss: 2.7353 - val_acc: 0.1148\n",
      "Epoch 9/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 2.8412 - acc: 0.1111 - val_loss: 2.6909 - val_acc: 0.1158\n",
      "Epoch 10/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 2.7746 - acc: 0.1206 - val_loss: 2.5188 - val_acc: 0.1403\n",
      "Epoch 11/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.8141 - acc: 0.1108 - val_loss: 2.7883 - val_acc: 0.1131\n",
      "Epoch 12/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.9242 - acc: 0.0972 - val_loss: 3.1658 - val_acc: 0.0559\n",
      "Epoch 13/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.0951 - acc: 0.0724 - val_loss: 2.9726 - val_acc: 0.0896\n",
      "Epoch 14/100\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 2.9347 - acc: 0.0841 - val_loss: 2.9549 - val_acc: 0.0788\n",
      "Epoch 15/100\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 2.7445 - acc: 0.1078 - val_loss: 2.6213 - val_acc: 0.1243\n",
      "Epoch 16/100\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 2.7103 - acc: 0.1193 - val_loss: 2.6784 - val_acc: 0.1209\n",
      "Epoch 17/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.6090 - acc: 0.1417 - val_loss: 3.1934 - val_acc: 0.0940\n",
      "Epoch 18/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 2.6486 - acc: 0.1415 - val_loss: 2.5768 - val_acc: 0.1192\n",
      "Epoch 19/100\n",
      "27455/27455 [==============================] - 3s 101us/step - loss: 2.8594 - acc: 0.1055 - val_loss: 2.6318 - val_acc: 0.1353\n",
      "Epoch 20/100\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 2.7774 - acc: 0.1207 - val_loss: 2.9588 - val_acc: 0.0810\n",
      "Epoch 21/100\n",
      "27455/27455 [==============================] - 3s 110us/step - loss: 2.8156 - acc: 0.1168 - val_loss: 2.5649 - val_acc: 0.1487\n",
      "Epoch 22/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 2.6749 - acc: 0.1362 - val_loss: 2.6800 - val_acc: 0.1333\n",
      "Epoch 23/100\n",
      "27455/27455 [==============================] - 3s 119us/step - loss: 2.6208 - acc: 0.1473 - val_loss: 2.2914 - val_acc: 0.2047\n",
      "Epoch 24/100\n",
      "27455/27455 [==============================] - 3s 126us/step - loss: 2.5825 - acc: 0.1544 - val_loss: 2.4967 - val_acc: 0.1683\n",
      "Epoch 25/100\n",
      "27455/27455 [==============================] - 3s 120us/step - loss: 2.5156 - acc: 0.1604 - val_loss: 2.4433 - val_acc: 0.1738\n",
      "Epoch 26/100\n",
      "27455/27455 [==============================] - 3s 116us/step - loss: 2.5160 - acc: 0.1722 - val_loss: 2.9895 - val_acc: 0.0666\n",
      "Epoch 27/100\n",
      "27455/27455 [==============================] - 3s 115us/step - loss: 2.5934 - acc: 0.1448 - val_loss: 2.3695 - val_acc: 0.2085\n",
      "Epoch 28/100\n",
      "27455/27455 [==============================] - 3s 114us/step - loss: 2.4561 - acc: 0.1727 - val_loss: 2.2211 - val_acc: 0.2464\n",
      "Epoch 29/100\n",
      "27455/27455 [==============================] - 3s 117us/step - loss: 2.3080 - acc: 0.2091 - val_loss: 2.6259 - val_acc: 0.1321\n",
      "Epoch 30/100\n",
      "27455/27455 [==============================] - 3s 112us/step - loss: 2.2159 - acc: 0.2421 - val_loss: 2.1956 - val_acc: 0.2582\n",
      "Epoch 31/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 2.1713 - acc: 0.2452 - val_loss: 2.0013 - val_acc: 0.2644\n",
      "Epoch 32/100\n",
      "27455/27455 [==============================] - 3s 116us/step - loss: 2.1552 - acc: 0.2545 - val_loss: 2.5990 - val_acc: 0.1566\n",
      "Epoch 33/100\n",
      "27455/27455 [==============================] - 3s 127us/step - loss: 2.2289 - acc: 0.2373 - val_loss: 2.5407 - val_acc: 0.1361\n",
      "Epoch 34/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 2.0658 - acc: 0.2742 - val_loss: 1.8929 - val_acc: 0.3223\n",
      "Epoch 35/100\n",
      "27455/27455 [==============================] - 3s 120us/step - loss: 1.9195 - acc: 0.3228 - val_loss: 1.8402 - val_acc: 0.3456\n",
      "Epoch 36/100\n",
      "27455/27455 [==============================] - 3s 114us/step - loss: 2.0431 - acc: 0.2963 - val_loss: 2.1480 - val_acc: 0.2772\n",
      "Epoch 37/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 1.8467 - acc: 0.3429 - val_loss: 1.7940 - val_acc: 0.3513\n",
      "Epoch 38/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 1.9633 - acc: 0.3275 - val_loss: 2.0916 - val_acc: 0.2896\n",
      "Epoch 39/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 2.2203 - acc: 0.2554 - val_loss: 2.1891 - val_acc: 0.2321\n",
      "Epoch 40/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 2.0676 - acc: 0.2852 - val_loss: 1.9448 - val_acc: 0.3263\n",
      "Epoch 41/100\n",
      "27455/27455 [==============================] - 3s 119us/step - loss: 1.8557 - acc: 0.3348 - val_loss: 1.7421 - val_acc: 0.3550\n",
      "Epoch 42/100\n",
      "27455/27455 [==============================] - 3s 116us/step - loss: 1.8318 - acc: 0.3659 - val_loss: 2.5371 - val_acc: 0.2319\n",
      "Epoch 43/100\n",
      "27455/27455 [==============================] - 3s 112us/step - loss: 1.9036 - acc: 0.3414 - val_loss: 1.6680 - val_acc: 0.3818\n",
      "Epoch 44/100\n",
      "27455/27455 [==============================] - 3s 110us/step - loss: 1.6979 - acc: 0.3970 - val_loss: 1.7262 - val_acc: 0.3600\n",
      "Epoch 45/100\n",
      "27455/27455 [==============================] - 3s 121us/step - loss: 1.7447 - acc: 0.4033 - val_loss: 1.7883 - val_acc: 0.3715\n",
      "Epoch 46/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 1.6481 - acc: 0.4256 - val_loss: 1.7894 - val_acc: 0.3792\n",
      "Epoch 47/100\n",
      "27455/27455 [==============================] - 4s 129us/step - loss: 1.5638 - acc: 0.4688 - val_loss: 2.4601 - val_acc: 0.2171\n",
      "Epoch 48/100\n",
      "27455/27455 [==============================] - 3s 127us/step - loss: 1.6249 - acc: 0.4493 - val_loss: 2.6038 - val_acc: 0.3709\n",
      "Epoch 49/100\n",
      "27455/27455 [==============================] - 4s 139us/step - loss: 1.8025 - acc: 0.4148 - val_loss: 1.4967 - val_acc: 0.4758\n",
      "Epoch 50/100\n",
      "27455/27455 [==============================] - 3s 125us/step - loss: 1.6165 - acc: 0.4732 - val_loss: 1.4202 - val_acc: 0.4789\n",
      "Epoch 51/100\n",
      "27455/27455 [==============================] - 3s 115us/step - loss: 1.3046 - acc: 0.5487 - val_loss: 1.0759 - val_acc: 0.6034\n",
      "Epoch 52/100\n",
      "27455/27455 [==============================] - 3s 126us/step - loss: 1.2318 - acc: 0.5787 - val_loss: 1.1933 - val_acc: 0.5825\n",
      "Epoch 53/100\n",
      "27455/27455 [==============================] - 3s 111us/step - loss: 1.3150 - acc: 0.5612 - val_loss: 0.9540 - val_acc: 0.6610\n",
      "Epoch 54/100\n",
      "27455/27455 [==============================] - 3s 123us/step - loss: 1.1430 - acc: 0.6144 - val_loss: 1.7309 - val_acc: 0.4853\n",
      "Epoch 55/100\n",
      "27455/27455 [==============================] - 3s 114us/step - loss: 1.0608 - acc: 0.6422 - val_loss: 0.8892 - val_acc: 0.6904\n",
      "Epoch 56/100\n",
      "27455/27455 [==============================] - 3s 114us/step - loss: 0.9732 - acc: 0.6814 - val_loss: 0.6312 - val_acc: 0.7775\n",
      "Epoch 57/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 1.4821 - acc: 0.5750 - val_loss: 1.8590 - val_acc: 0.5025\n",
      "Epoch 58/100\n",
      "27455/27455 [==============================] - 3s 112us/step - loss: 1.2761 - acc: 0.5866 - val_loss: 1.2526 - val_acc: 0.6211\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27455/27455 [==============================] - 3s 113us/step - loss: 0.9564 - acc: 0.6857 - val_loss: 0.6436 - val_acc: 0.7692\n",
      "Epoch 60/100\n",
      "27455/27455 [==============================] - 3s 110us/step - loss: 1.3652 - acc: 0.5622 - val_loss: 1.0098 - val_acc: 0.6454\n",
      "Epoch 61/100\n",
      "27455/27455 [==============================] - 3s 115us/step - loss: 1.0703 - acc: 0.6430 - val_loss: 3.2939 - val_acc: 0.2946\n",
      "Epoch 62/100\n",
      "27455/27455 [==============================] - 4s 133us/step - loss: 1.1587 - acc: 0.6291 - val_loss: 0.7125 - val_acc: 0.7605\n",
      "Epoch 63/100\n",
      "27455/27455 [==============================] - 3s 126us/step - loss: 0.8786 - acc: 0.7196 - val_loss: 0.9391 - val_acc: 0.7021\n",
      "Epoch 64/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 0.7008 - acc: 0.7773 - val_loss: 2.6483 - val_acc: 0.5580\n",
      "Epoch 65/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 7.8201 - acc: 0.0794 - val_loss: 3.2943 - val_acc: 0.0383\n",
      "Epoch 66/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.2689 - acc: 0.0407 - val_loss: 3.2433 - val_acc: 0.0383\n",
      "Epoch 67/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 3.2310 - acc: 0.0401 - val_loss: 3.2152 - val_acc: 0.0383\n",
      "Epoch 68/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.2093 - acc: 0.0452 - val_loss: 3.1991 - val_acc: 0.0510\n",
      "Epoch 69/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 3.1967 - acc: 0.0466 - val_loss: 3.1897 - val_acc: 0.0510\n",
      "Epoch 70/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.1893 - acc: 0.0466 - val_loss: 3.1844 - val_acc: 0.0510\n",
      "Epoch 71/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 3.1852 - acc: 0.0456 - val_loss: 3.1815 - val_acc: 0.0510\n",
      "Epoch 72/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 3.1829 - acc: 0.0466 - val_loss: 3.1800 - val_acc: 0.0485\n",
      "Epoch 73/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 3.1817 - acc: 0.0471 - val_loss: 3.1791 - val_acc: 0.0485\n",
      "Epoch 74/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 3.1810 - acc: 0.0465 - val_loss: 3.1786 - val_acc: 0.0485\n",
      "Epoch 75/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.1805 - acc: 0.0464 - val_loss: 3.1782 - val_acc: 0.0485\n",
      "Epoch 76/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 3.1802 - acc: 0.0471 - val_loss: 3.1780 - val_acc: 0.0485\n",
      "Epoch 77/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 3.1799 - acc: 0.0471 - val_loss: 3.1777 - val_acc: 0.0485\n",
      "Epoch 78/100\n",
      "27455/27455 [==============================] - 3s 107us/step - loss: 3.1797 - acc: 0.0467 - val_loss: 3.1776 - val_acc: 0.0485\n",
      "Epoch 79/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 3.1795 - acc: 0.0469 - val_loss: 3.1774 - val_acc: 0.0485\n",
      "Epoch 80/100\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 3.1794 - acc: 0.0471 - val_loss: 3.1772 - val_acc: 0.0485\n",
      "Epoch 81/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 3.1792 - acc: 0.0468 - val_loss: 3.1771 - val_acc: 0.0485\n",
      "Epoch 82/100\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 3.1791 - acc: 0.0469 - val_loss: 3.1770 - val_acc: 0.0485\n",
      "Epoch 83/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.1790 - acc: 0.0471 - val_loss: 3.1768 - val_acc: 0.0485\n",
      "Epoch 84/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 3.1788 - acc: 0.0471 - val_loss: 3.1767 - val_acc: 0.0485\n",
      "Epoch 85/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 3.1787 - acc: 0.0468 - val_loss: 3.1766 - val_acc: 0.0485\n",
      "Epoch 86/100\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 3.1786 - acc: 0.0468 - val_loss: 3.1766 - val_acc: 0.0485\n",
      "Epoch 87/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 3.1785 - acc: 0.0468 - val_loss: 3.1764 - val_acc: 0.0485\n",
      "Epoch 88/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.1784 - acc: 0.0471 - val_loss: 3.1764 - val_acc: 0.0485\n",
      "Epoch 89/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 3.1784 - acc: 0.0471 - val_loss: 3.1763 - val_acc: 0.0485\n",
      "Epoch 90/100\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 3.1783 - acc: 0.0469 - val_loss: 3.1762 - val_acc: 0.0485\n",
      "Epoch 91/100\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 3.1782 - acc: 0.0471 - val_loss: 3.1761 - val_acc: 0.0485\n",
      "Epoch 92/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 3.1781 - acc: 0.0471 - val_loss: 3.1760 - val_acc: 0.0485\n",
      "Epoch 93/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 3.1781 - acc: 0.0468 - val_loss: 3.1760 - val_acc: 0.0485\n",
      "Epoch 94/100\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 3.1780 - acc: 0.0471 - val_loss: 3.1759 - val_acc: 0.0485\n",
      "Epoch 95/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 3.1779 - acc: 0.0456 - val_loss: 3.1758 - val_acc: 0.0485\n",
      "Epoch 96/100\n",
      "27455/27455 [==============================] - 3s 113us/step - loss: 3.1779 - acc: 0.0468 - val_loss: 3.1758 - val_acc: 0.0485\n",
      "Epoch 97/100\n",
      "27455/27455 [==============================] - 3s 112us/step - loss: 3.1778 - acc: 0.0464 - val_loss: 3.1757 - val_acc: 0.0485\n",
      "Epoch 98/100\n",
      "27455/27455 [==============================] - 3s 108us/step - loss: 3.1778 - acc: 0.0471 - val_loss: 3.1756 - val_acc: 0.0485\n",
      "Epoch 99/100\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 3.1777 - acc: 0.0466 - val_loss: 3.1756 - val_acc: 0.0485\n",
      "Epoch 100/100\n",
      "27455/27455 [==============================] - 3s 107us/step - loss: 3.1777 - acc: 0.0471 - val_loss: 3.1755 - val_acc: 0.0485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210cd609630>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(300, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "modelf.add(Dense(250, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(200, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(150, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(100, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(50, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(25, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(25, init='uniform', activation='softmax'))\n",
    "modelf.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelf.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve que luego del epoch 50 comienza a empeorar por lo que se disminuye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(250, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(150, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Clau\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27455 samples, validate on 6864 samples\n",
      "Epoch 1/50\n",
      "27455/27455 [==============================] - 3s 121us/step - loss: 3.2056 - acc: 0.0457 - val_loss: 3.1836 - val_acc: 0.0447\n",
      "Epoch 2/50\n",
      "27455/27455 [==============================] - 3s 124us/step - loss: 3.1743 - acc: 0.0471 - val_loss: 3.1365 - val_acc: 0.0501\n",
      "Epoch 3/50\n",
      "27455/27455 [==============================] - 3s 117us/step - loss: 3.1702 - acc: 0.0489 - val_loss: 3.1639 - val_acc: 0.0475\n",
      "Epoch 4/50\n",
      "27455/27455 [==============================] - 3s 99us/step - loss: 3.1515 - acc: 0.0593 - val_loss: 3.1273 - val_acc: 0.0473\n",
      "Epoch 5/50\n",
      "27455/27455 [==============================] - 3s 99us/step - loss: 3.1522 - acc: 0.0537 - val_loss: 3.1728 - val_acc: 0.0392\n",
      "Epoch 6/50\n",
      "27455/27455 [==============================] - 3s 97us/step - loss: 3.1114 - acc: 0.0608 - val_loss: 4.2192 - val_acc: 0.0494\n",
      "Epoch 7/50\n",
      "27455/27455 [==============================] - 2s 90us/step - loss: 3.0872 - acc: 0.0674 - val_loss: 3.1450 - val_acc: 0.0543\n",
      "Epoch 8/50\n",
      "27455/27455 [==============================] - 3s 97us/step - loss: 3.0015 - acc: 0.0864 - val_loss: 2.9054 - val_acc: 0.0874\n",
      "Epoch 9/50\n",
      "27455/27455 [==============================] - 3s 94us/step - loss: 2.9002 - acc: 0.0947 - val_loss: 2.7800 - val_acc: 0.1225\n",
      "Epoch 10/50\n",
      "27455/27455 [==============================] - 3s 96us/step - loss: 2.9186 - acc: 0.0906 - val_loss: 2.9131 - val_acc: 0.0954\n",
      "Epoch 11/50\n",
      "27455/27455 [==============================] - 3s 101us/step - loss: 2.7815 - acc: 0.1085 - val_loss: 2.8479 - val_acc: 0.1158\n",
      "Epoch 12/50\n",
      "27455/27455 [==============================] - 3s 93us/step - loss: 2.7562 - acc: 0.1167 - val_loss: 2.6308 - val_acc: 0.1218\n",
      "Epoch 13/50\n",
      "27455/27455 [==============================] - 3s 99us/step - loss: 2.9619 - acc: 0.0833 - val_loss: 2.7738 - val_acc: 0.1243\n",
      "Epoch 14/50\n",
      "27455/27455 [==============================] - 3s 99us/step - loss: 2.8046 - acc: 0.1105 - val_loss: 2.7698 - val_acc: 0.0864\n",
      "Epoch 15/50\n",
      "27455/27455 [==============================] - 2s 90us/step - loss: 2.7568 - acc: 0.1075 - val_loss: 2.8677 - val_acc: 0.1266\n",
      "Epoch 16/50\n",
      "27455/27455 [==============================] - 3s 98us/step - loss: 2.7966 - acc: 0.1080 - val_loss: 2.8169 - val_acc: 0.1093\n",
      "Epoch 17/50\n",
      "27455/27455 [==============================] - 3s 92us/step - loss: 2.7322 - acc: 0.1166 - val_loss: 2.5132 - val_acc: 0.1447\n",
      "Epoch 18/50\n",
      "27455/27455 [==============================] - 3s 93us/step - loss: 2.7401 - acc: 0.1265 - val_loss: 2.6094 - val_acc: 0.1186\n",
      "Epoch 19/50\n",
      "27455/27455 [==============================] - 3s 94us/step - loss: 2.6031 - acc: 0.1371 - val_loss: 2.5068 - val_acc: 0.1470\n",
      "Epoch 20/50\n",
      "27455/27455 [==============================] - 3s 94us/step - loss: 2.8723 - acc: 0.0972 - val_loss: 3.2770 - val_acc: 0.0645\n",
      "Epoch 21/50\n",
      "27455/27455 [==============================] - 3s 98us/step - loss: 2.6701 - acc: 0.1268 - val_loss: 2.5791 - val_acc: 0.1345\n",
      "Epoch 22/50\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 2.5248 - acc: 0.1551 - val_loss: 2.3026 - val_acc: 0.1986\n",
      "Epoch 23/50\n",
      "27455/27455 [==============================] - 3s 101us/step - loss: 2.4090 - acc: 0.1736 - val_loss: 2.2264 - val_acc: 0.2002\n",
      "Epoch 24/50\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 2.5276 - acc: 0.1567 - val_loss: 2.7240 - val_acc: 0.1254\n",
      "Epoch 25/50\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 2.4927 - acc: 0.1646 - val_loss: 2.3194 - val_acc: 0.1706\n",
      "Epoch 26/50\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.5527 - acc: 0.1478 - val_loss: 2.6718 - val_acc: 0.1320\n",
      "Epoch 27/50\n",
      "27455/27455 [==============================] - 3s 97us/step - loss: 2.4428 - acc: 0.1684 - val_loss: 2.5132 - val_acc: 0.1480\n",
      "Epoch 28/50\n",
      "27455/27455 [==============================] - 3s 101us/step - loss: 2.7423 - acc: 0.1261 - val_loss: 3.0591 - val_acc: 0.1017\n",
      "Epoch 29/50\n",
      "27455/27455 [==============================] - 3s 98us/step - loss: 2.9117 - acc: 0.1029 - val_loss: 2.6706 - val_acc: 0.1321\n",
      "Epoch 30/50\n",
      "27455/27455 [==============================] - 3s 98us/step - loss: 2.8931 - acc: 0.0992 - val_loss: 2.6377 - val_acc: 0.1399\n",
      "Epoch 31/50\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 2.7259 - acc: 0.1201 - val_loss: 2.5744 - val_acc: 0.1412\n",
      "Epoch 32/50\n",
      "27455/27455 [==============================] - 3s 100us/step - loss: 2.6031 - acc: 0.1454 - val_loss: 2.6210 - val_acc: 0.1613\n",
      "Epoch 33/50\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 2.8632 - acc: 0.1051 - val_loss: 2.7610 - val_acc: 0.0964\n",
      "Epoch 34/50\n",
      "27455/27455 [==============================] - 3s 115us/step - loss: 2.6637 - acc: 0.1377 - val_loss: 2.5737 - val_acc: 0.1385\n",
      "Epoch 35/50\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.6559 - acc: 0.1399 - val_loss: 2.8734 - val_acc: 0.1215\n",
      "Epoch 36/50\n",
      "27455/27455 [==============================] - 3s 115us/step - loss: 2.6652 - acc: 0.1428 - val_loss: 2.6961 - val_acc: 0.1026\n",
      "Epoch 37/50\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.7288 - acc: 0.1292 - val_loss: 2.6315 - val_acc: 0.1362\n",
      "Epoch 38/50\n",
      "27455/27455 [==============================] - 3s 120us/step - loss: 2.6312 - acc: 0.1405 - val_loss: 2.6920 - val_acc: 0.1149\n",
      "Epoch 39/50\n",
      "27455/27455 [==============================] - 3s 114us/step - loss: 2.5074 - acc: 0.1648 - val_loss: 2.8909 - val_acc: 0.0874\n",
      "Epoch 40/50\n",
      "27455/27455 [==============================] - 3s 112us/step - loss: 2.6719 - acc: 0.1487 - val_loss: 2.3350 - val_acc: 0.1843\n",
      "Epoch 41/50\n",
      "27455/27455 [==============================] - 3s 106us/step - loss: 2.8683 - acc: 0.1031 - val_loss: 2.6379 - val_acc: 0.1301\n",
      "Epoch 42/50\n",
      "27455/27455 [==============================] - 3s 124us/step - loss: 2.6150 - acc: 0.1408 - val_loss: 2.4651 - val_acc: 0.1665\n",
      "Epoch 43/50\n",
      "27455/27455 [==============================] - 3s 110us/step - loss: 2.4354 - acc: 0.1788 - val_loss: 2.2824 - val_acc: 0.2061\n",
      "Epoch 44/50\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 2.3670 - acc: 0.1866 - val_loss: 2.6843 - val_acc: 0.1432\n",
      "Epoch 45/50\n",
      "27455/27455 [==============================] - 3s 109us/step - loss: 2.3241 - acc: 0.2016 - val_loss: 2.4536 - val_acc: 0.1983\n",
      "Epoch 46/50\n",
      "27455/27455 [==============================] - 3s 104us/step - loss: 2.2455 - acc: 0.2195 - val_loss: 2.1582 - val_acc: 0.2258\n",
      "Epoch 47/50\n",
      "27455/27455 [==============================] - 3s 103us/step - loss: 2.2560 - acc: 0.2278 - val_loss: 4.1660 - val_acc: 0.0932\n",
      "Epoch 48/50\n",
      "27455/27455 [==============================] - 3s 102us/step - loss: 2.2783 - acc: 0.2228 - val_loss: 2.1469 - val_acc: 0.2188\n",
      "Epoch 49/50\n",
      "27455/27455 [==============================] - 3s 100us/step - loss: 2.1288 - acc: 0.2551 - val_loss: 2.5783 - val_acc: 0.1885\n",
      "Epoch 50/50\n",
      "27455/27455 [==============================] - 3s 105us/step - loss: 2.2097 - acc: 0.2436 - val_loss: 2.0483 - val_acc: 0.2411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210ce4d0fd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelf = Sequential()\n",
    "modelf.add(Dense(300, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "modelf.add(Dense(250, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(200, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(150, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(100, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(50, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(25, init='uniform', activation='relu'))\n",
    "modelf.add(Dense(25, init='uniform', activation='softmax'))\n",
    "modelf.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelf.fit(x_tr.values, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene en este caso un resultado de acurracy 0.2436"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "Para la mejor red entrenada anteriormente construya la matriz de confusión de las distintas clases, para\n",
    "asi visualizar cúales son las clases más difíciles de clasificar y con cuáles se confunden. Comente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "val = modelf.predict(x_v_scaled)\n",
    "conf_mat = confusion_matrix(y_v,val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)\n",
    "Se entrena una SVM no lineal con los pixeles sin procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC as SVC\n",
    "from sklearn.metrics import f1_score\n",
    "def esevem(k):\n",
    "    model = SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False)\n",
    "    model.set_params(C=k,)\n",
    "    model.fit(x_tr,y_tr)\n",
    "    y_pred=model.predict(x_v)\n",
    "    y_true=y_v\n",
    "    val=f1_score(y_true, y_pred)\n",
    "    return val\n",
    "\n",
    "s=esevem(i)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f)\n",
    "Se entrena un árbol de clasificación sobre los pixeles con pre-procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 24.   8.  18. ...,  16.  18.  16.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-cc92cbbe27da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marbol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m  \u001b[0ma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmaxi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-cc92cbbe27da>\u001b[0m in \u001b[0;36marbol\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0my_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_v_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# average=\"micro\" average=\"macro\" average='weighted'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \"\"\"\n\u001b[0;32m    411\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tree_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    375\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# To ensure that array flags are maintained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 24.   8.  18. ...,  16.  18.  16.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "def arbol(t):\n",
    "    model= Tree(random_state=4)\n",
    "    model.set_params(criterion='gini',splitter='best',max_depth=t)\n",
    "    model.fit(x_tr_scaled, y_tr)\n",
    "    y_true =  y_v\n",
    "    y_pred =  model.predict(x_v_scaled)\n",
    "    a=model.score(y_true,y_pred) # average=\"micro\" average=\"macro\" average='weighted'\n",
    "    return a\n",
    "maxi=0\n",
    "for i in range(1,50):\n",
    "    a=arbol(i)\n",
    "    if  a > maxi:\n",
    "        maxi=a\n",
    "        save=i\n",
    "print (a,save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
